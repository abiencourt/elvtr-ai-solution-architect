{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ede245-8bd8-4ca3-8922-c3b725ba05ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AISA Capstone 2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc35067-b7cf-438c-81ff-1eb4ed6a9725",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook provides an environment for you to build your intuition on the steps to take when developing a high quality Retrieval Augmented Generation (RAG) solution. \n",
    "RAG solutions retrieve data before calling the large language model (LLM) to generate an answer. \n",
    "The retrieved data is used to augment the prompt to the LLM by adding the relevant retrieved data in context. \n",
    "Any RAG solution is only as good as the quality of the data retrieval process, and this is the particular focus of this notebook, and the accompanying questions and tasks.\n",
    "\n",
    "The RAG solution developed here is enabled by the Llamaindex framework. This is a popular framework in the industry for developing RAG and Agent based solutions. In addition to providing a core set of tools for orchestration of RAG and Agent workflows, there is broad integration with a variety of platforms for model inference (LLM, embedding, ...), and, importantly, tooling for solution evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4d6f9-7cc1-416b-b035-8403ca7bc8f0",
   "metadata": {},
   "source": [
    "## Prerequisites for running the notebook\n",
    "- That you have granted access to the Bedrock models that you are going to use, in the region (**us-west-2**) where you are going to use Bedrock - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html)\n",
    "- Your SageMakerExecutionRole has permissions to invoke Bedrock models - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-prereq.html)\n",
    "- This notebook has been tested with SageMaker Notebook Instance running a `conda_python3` kernel\n",
    "- The AWS region set for Amazon Bedrock use, needs to be in a region where the models being used are 1/ available, and 2/ enabled for use. This notebook was tested with Bedrock region `us-west-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514f985-a013-48e9-acd9-73d4eeb137f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementation\n",
    "This notebook uses llamaindex to define and execute the RAG solution. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: e.g. Anthropic Claude Haiku available through Amazon Bedrock\n",
    "\n",
    "  LLMs are used in the notebook for 1/ RAG response generation, to show the overall RAG workflow in actions, and 2/ for generating test questions on the indexed content (llamaindex nodes) for retrieval evaluation.\n",
    "  \n",
    "- **Text Embeddings Model**: e.g. Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This embedding model is used to generate semantic vector representations of the content (llamaindex nodes) to be stored and the questions input to the RAG solution.\n",
    "  \n",
    "- **Document Loader**: SimpleDirectoryReader (Llamaindex)\n",
    "\n",
    "  Before your chosen LLM can act on your data you need to load it. The way LlamaIndex does this is via data connectors, also called 'Reader'. Data connectors ingest data from different data sources and format the data into Document objects. A Document is a collection of data (currently text, and in future, images and audio) and metadata about that data.\n",
    "  \n",
    "  This implementation use SimpleDirectoryReader, which creates documents out of every file in a given directory. It can read a variety of formats including Markdown, PDFs, Word documents, and PowerPoint decks.\n",
    "\n",
    "- **Vector Store**: VectorIndex (Llamaindex)\n",
    "\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "  \n",
    "  LlamaIndex abstracts the underlying vector database storage implementation with a VectorIndex class. This warps the Index, which is a data structure composed of Document objects, designed to enable querying by an LLM. The Index is designed to be complementary to your querying strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe5196-2edf-4f54-be53-12bc4d70e3ea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da34a35-29ee-4c26-8398-7e1bfe7c58f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293136b-753b-4774-95bc-cd965a38b13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install required Python modules for constructing the RAG solution.\n",
    "You only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c358c2-f00d-4582-867a-1ae49bfa9354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    llama-index \\\n",
    "    llama-index-llms-bedrock \\\n",
    "    llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724a2d-6686-4177-baa1-3fed868170d5",
   "metadata": {},
   "source": [
    "Download the default RAG test source data to our target source_docs directory. \n",
    "You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190bed5-26d3-4897-a270-74087c9af1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_docs_dir = './source_docs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d14fc8-2f54-4a24-9ad4-9e84678e9495",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following creates the source_docs directory and downloads a document to that directory. The contents of this directory, \n",
    "initially the document that is downloaded here, will be used in the steps that follow.\n",
    "\n",
    "After running this notebook in its entirity and reviewing its operation, delete this content and add your own content to the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9c7c-3e23-40ba-a7c1-b852d083aa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and load data\n",
    "!mkdir -p {source_docs_dir}\n",
    "!wget --no-check-certificate 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O {source_docs_dir}'/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab5b10-dcb8-423c-b1ef-d39a656f6b41",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import auxilliarty modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f69ef6-0453-465f-adc0-3180b7f41352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3  # AWS SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d3264-223c-4902-8e21-6886e25a52b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is required when running within a jupyter notebook, otherwise you will get errors when llamaindex modules run\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b618759-983d-4d7e-8bf6-840b355b5609",
   "metadata": {},
   "source": [
    "Import required Python modules for constructing and evaluating the RAG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025ceaf-9699-43ba-9369-773612652b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    RetrieverEvaluator,\n",
    "    generate_question_context_pairs,\n",
    ")\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Response,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66520-b9b5-4fdc-a00b-45d23734bfdb",
   "metadata": {},
   "source": [
    "## Configure the models that will be used for the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b8643-0fc7-4984-bfb4-820b92ce0e99",
   "metadata": {},
   "source": [
    "**Note**: By default this notebook with use the `us-west-2` region. This region has support for the models used in this notebook. You should not need to change this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946049d-010b-4485-83ce-08425c254261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-west-2\"\n",
    "# AWS_REGION = \"us-east-1\"  # this is an alternative setting to use if desired "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6109-6dba-4312-951a-af27765774b3",
   "metadata": {},
   "source": [
    "Define the set of Bedrock model IDs that we that we'll use when developing and testing our solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8d3b8-0a47-45db-9b2f-3265d0c795f0",
   "metadata": {},
   "source": [
    "Establish a connection to the Amazon Bedrock service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ca8a1-d4b7-463e-8770-76bf83a37246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d038da-c7fb-4c07-957c-c23a1effe13e",
   "metadata": {},
   "source": [
    "### Configure the target embeddings models for use with Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2a047-82b9-4ee9-bbcc-4ab33d9a07f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_multilingual_v1_id = \"amazon.titan-embed-text-v1\"\n",
    "titan_text_embeddings_multilingual_v2_id = \"amazon.titan-embed-text-v2:0\"\n",
    "cohere_text_embeddings_english_id = \"cohere.embed-english-v3\"\n",
    "cohere_text_embeddings_multilingual_id = \"cohere.embed-multilingual-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ca6ba-721d-4af6-a25f-0026218373c5",
   "metadata": {},
   "source": [
    "Configure our chosen embeddings model for use with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac101f7-1f62-4720-9a94-64bdce9102de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_v2 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v2_id,region_name=AWS_REGION)\n",
    "titan_text_embeddings_v1 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v1_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_english = BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_multilingual= BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac963474-ee9e-47f4-ae7f-5896b3ee805d",
   "metadata": {},
   "source": [
    "### Configure the target LLMs for use with Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66fcc-a3c2-4acd-9004-53d96c083391",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following Mistral models produce good questions for evaluation. The Titan model produces questions of lesser quality \n",
    "and sometimes not in the format needed by the tools. \n",
    "\n",
    "**Note** Most Bedrock LLMs do note produce questions in a format that can be directly used for evaluation with the tooling as it is configured in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d60343-0fad-4fad-a6a0-1a28a60c5539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id=\"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "titan_text_express_id = \"amazon.titan-text-express-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66c9c9-752f-425c-b783-c5951c47d95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the parameters to be applied when invoking the model\n",
    "model_kwargs_llm = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 200,\n",
    "    \"max_tokens\": 4096 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223cc97-dc5b-40f4-8acb-92be61e03885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_mistral7b = Bedrock(model=instruct_mistral7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_mixtral8x7b = Bedrock(model=instruct_mixtral8x7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_titan_express = Bedrock(model=titan_text_express_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babd1c3-cc16-4914-ba7a-83185c56a418",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the embeddings model to use for the cells that follow\n",
    "\n",
    "The embeddings model is a critical choice for the accuracy of your RAG solution.\n",
    "Experiment with the options here to see which is best for your content.\n",
    "If you want more, test with further alternatives. There are many that are readily supported by llama_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d75b4-f9ef-4398-9214-15920b917ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL 01\n",
    "\n",
    "embed_model = titan_text_embeddings_v1\n",
    "# embed_model = titan_text_embeddings_v2\n",
    "# embed_model = cohere_text_embeddings_english\n",
    "# embed_model = cohere_text_embeddings_multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d0f7-6744-41e8-a62b-8f5624fe54f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the LLM to use for the cells that follow\n",
    "The LLM will be used for question generation and RAG answer generation in this notebook as it is currently configured.\n",
    "The default value llm_mistral7b works well with the code and should be used if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea65f7-1428-41c0-850d-c1eb176c41e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = llm_mistral7b\n",
    "# llm_model = llm_mixtral8x7b\n",
    "# llm_model = llm_titan_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbe67d-5908-4d3f-8c0a-831391a4ad5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set LlamaIndex default model settings to what was set in the cells above\n",
    "Settings.embed_model = embed_model \n",
    "Settings.llm = llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46229098-ce14-4834-a863-2035547a53ab",
   "metadata": {},
   "source": [
    "## Read in the documents for adding to our data store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fc1fd-f34b-41a1-9d88-e4647a521273",
   "metadata": {},
   "source": [
    "Read in the documents in the 'data/source_docs' directory into a structure ready for use by llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bd6d1-2454-45ff-a33e-fcbf3ea77867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(source_docs_dir)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b952f0-8f8a-4920-ad35-e8abe08fc429",
   "metadata": {},
   "source": [
    "Quick check here to see that all of your documents were read. The count should match the number of pages in the documents in source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752782ee-1d5c-41d7-8266-c1a6c9194a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261170a-c9db-4f59-b391-9e5039397e26",
   "metadata": {},
   "source": [
    "## Create and run the document ingestion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1fda-bc75-42a1-be1b-54c29a624abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell defines two different document ingestion pipelines. \n",
    "If you have time, test using both of these, and create you own and test with that also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773aa337-a276-441a-92be-190a2d9f37e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define two transformation for the ingestion pipelines for initial experimentation\n",
    "\n",
    "transformations_00=[\n",
    "        TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=100),\n",
    "        embed_model,\n",
    "    ]\n",
    "\n",
    "transformations_01=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=100),\n",
    "        TitleExtractor(),\n",
    "        embed_model,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b013b-f07e-441d-8396-dab15325e7d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the data ingestion pipeline for processing the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134f8fb-03eb-4167-9063-4fcd08ed7643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL 02\n",
    "\n",
    "# create the pipeline with one of the transformation configurations defined above\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations_00)\n",
    "# pipeline = IngestionPipeline(transformations=transformations_01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314c658-de88-432b-98bc-08a5ccf7bee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the configured ingestion pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6cf10-0dcc-40eb-a659-3b70cfb4a939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(f\"number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331bd97-75af-432b-a77c-a82be092585d",
   "metadata": {
    "tags": []
   },
   "source": [
    "This may make test analysis easier. It is none essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b464f0b-b123-4a12-b85f-8718fcaee4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default, the node ids are set to random uuids. \n",
    "# To ensure same id's per run, we manually set them to consistent sequential numbers.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb0df-bf84-4630-87b0-ba8fae23f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate that node has an embedding associated with it\n",
    "for idx, node in enumerate(nodes):\n",
    "    if node.id_ == \"node_0\":\n",
    "        print(node.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620ecd1-c357-49a0-8921-7de63903f870",
   "metadata": {},
   "source": [
    "## Create the VectorIndex \n",
    "This creates our vector database, in memory in this case,  using the nodes that were created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25602778-44d3-4adf-b3e2-68b679feeea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1160-b3e8-445d-8680-fa9d46faea06",
   "metadata": {},
   "source": [
    "## Test that we have a valid starting point for our evaluation\n",
    "We run a quick system test with the defaul llama_index RAG workflow with a question that is relevant to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3006bf-14de-4c74-93dd-3a584bcb5be9",
   "metadata": {},
   "source": [
    "Instantiate a query engine object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3b4be-5fe1-4789-b7b9-bf17cb6a9f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c163ad-7a8c-40e1-a9f4-8caac25e8dc1",
   "metadata": {},
   "source": [
    "Specify a question that has can be answered by the document(s) that have been ingested. For the default document, the following is a valid question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a8e95-22ae-45e2-9f9b-d0734d6a4bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"What did the author do growing up?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479cecf-4725-45f6-be71-7cd5bf4cf915",
   "metadata": {},
   "source": [
    "Run the default RAG pipeline with the example query. This should give a meaningful result. Don't worry if the answer is overly verbose, etc. We'll fix that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8af86-4544-4286-be1a-a32b7fe55c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(example_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06687f-c443-4bdb-8e70-25686fbd3564",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127b15d-cb49-43ab-8f8e-a908915eb6c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the retrieval accuracy of the VectorIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85730c7c-81f2-493c-8fe0-3a59204e2b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a set of question and node (context) pairs to drive the tests that follow\n",
    "This uses the llm that give the methods and the document data stored in the nodes (created during document ingestion)\n",
    "\n",
    "This will make many calls to the specified LLM (num_questions_per_chunk * number of nodes). This will likely be throttled by Bedrock. The llama_index API will work through the throttling except in extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0e486-180b-42e3-8ca3-852e73690c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qa_dataset = generate_question_context_pairs(nodes, num_questions_per_chunk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9794d-4890-42ca-867b-5558955932e9",
   "metadata": {},
   "source": [
    "Take a look at the sample queries generated. This should show a meaningful questions related to your document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29a7fc-22b4-4ff4-98d6-764280755a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in list(qa_dataset.queries.items()):\n",
    "    print(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a4af-f49d-4eea-82ca-e7d5cca9b990",
   "metadata": {},
   "source": [
    "## Instantiate a retriever against the index for testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fad7c-f10c-46a3-8a9a-6d3e5c25b64d",
   "metadata": {},
   "source": [
    "### Set the number of items to return from the Retriever\n",
    "This is a trade-off item, more returned content is not always better. Consider how this may impact your pipeline and evaluation results and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66407c-ea89-4c90-8be0-1bc70c2700fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL: 03\n",
    "\n",
    "number_of_items_to_return = 2\n",
    "# number_of_items_to_return = 3\n",
    "# number_of_items_to_return = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6db691-6af4-4856-8ad2-604c0c6f629c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=number_of_items_to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab82df-ebaa-4a2f-a98b-298198672f26",
   "metadata": {},
   "source": [
    "Run a quick system test on the retriever and check that the output nodes look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf0400-0e17-4d7f-aee7-95546b8eaa4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"What did the author do growing up?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6327e-46ca-4e1d-9979-11c86471d784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(example_query)\n",
    "print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f47e2-1e69-4a8e-a26c-886ef2e4dd3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the Quality of Retrieval from the VectorIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768664c-f6da-4870-a882-1affb219a2d1",
   "metadata": {},
   "source": [
    "Instantiate a RetrieverEvaluator with the metrics that we want to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99063386-621d-4ee7-93e6-17ab95e49839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(metrics, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffc053-eab3-42f8-8b21-d8abcc5e19e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on a single query \n",
    "# The output is verbose, but may be useful for looking at specific results\n",
    "\n",
    "query_id = 1  # change this to math the query id of interest\n",
    "\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[query_id]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf98b03-9305-46f3-a9fe-52cbd1d1cd97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to see detail on which nodes were returned, etc, we can look at the whole returned object\n",
    "eval_result_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f05b3-d20b-41e7-88b8-27a020ce3041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run evaulation on the entire test dataset (autogenerated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e410697-2972-4b9f-8f8d-94d477afdc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20614f76-9896-4253-8a5e-e101a1f082ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    precision = full_df[\"precision\"].mean()\n",
    "    recall = full_df[\"recall\"].mean()\n",
    "\n",
    "\n",
    "    metric_df = pd.DataFrame({\"retrievers\": [name], \n",
    "                              \"hit_rate\": [hit_rate], \"mrr\": [mrr],\n",
    "                              \"precision\": [precision], \"recall\": [recall],\n",
    "                             })\n",
    "    return metric_df, full_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1159cf-271e-46b2-8264-4bd89b39b648",
   "metadata": {},
   "source": [
    "### Top-level Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42016d73-4f1a-4d09-adc0-5bd8a8ffda85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary, detail = display_results(f\"top-{number_of_items_to_return} eval\", eval_results)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec8877-bec8-4c50-b2f0-67ad6b7b5dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally, look at the detailed, question by question metrics:\n",
    "detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d393f-7b77-4de0-86a1-569fb4d6de5c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f08a3-317c-46fc-8ccb-3f0b133897aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fbf95-39a4-43e4-b1ce-8296f28c8737",
   "metadata": {},
   "source": [
    "**01** Run the notebook in full, with the default, provided, document set (one document in .txt file).\n",
    "\n",
    "Copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell.\n",
    "\n",
    "Save the notebook and rename is as `capstone-02-01-first-run.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a91685-af8e-40c3-9d75-8e427a227043",
   "metadata": {},
   "source": [
    "**02** Replace the default document with one of more of your own documents and re-run all the cells.\n",
    "\n",
    "- First make a copy of the first notebook and paste it into the same directory.\n",
    "- Delete the original content on source_docs\n",
    "- Delete the cell in the notebook that will download the original content again to source_docs\n",
    "- Upload your content to source_docs\n",
    "- Initially test with a small set - 20 to 40 nodes aka document chunks - to save you time as you experiment. This may mean that you delete some of your content, to reduce its size. \n",
    "- Run the whole notebook with your small set of  test documents and observe the results.\n",
    "\n",
    "Wrapping up:\n",
    "- Copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell.\n",
    "- The top-level results returned are your `baseline` results. As you experiment with different configurations, in the following assignment tasks, you will see the route to improved accuracy from this baseline.\n",
    "- Describe your test dataset in this cell. How many documents, how many chunks, what is the topic of the content, what is the lanuage.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-02-my-data-baseline.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc93a6-9787-4fe3-aa06-15d06fb3b668",
   "metadata": {},
   "source": [
    "**03** Using your data experiment with different embedding models\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-02-my-data-baseline.ipynb`, and paste it into the same directory.\n",
    "- Change which embedding model is going to be used from the original setting. \n",
    "There are three other options prepared in this notebook, see `KEY CELL 01`. \n",
    "- Run all the following cells in the notebook and note the in accuracy metrics for the selected model.\n",
    "- Repeat with one or more of the embeddings models, noting the accuracy metrics for the selected model, each time.\n",
    "Note: Changing the embeddings model will change the accuracy of the retriever. Exactly how much will depend on your content.\n",
    "\n",
    "Wrapping up:\n",
    "- For each embeddings model that you experiment with, copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell, along with an indication as to which embeddings model was being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which model was best, did you see a major improvement in the metrics, can you suggest a reason for why the best embeddings model is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-03-my-data-embeddings.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b739f-5af8-4719-8d69-ae70af9ed9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**04** Using your data experiment with different ingestion pipelines\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-03-my-data-embeddings.ipynb`, and paste it into the same directory.\n",
    "- Make sure the the notebook is configured to use the best performing embeddings model.\n",
    "- Change which pipeline is going to be used from the original setting. \n",
    "There is one other options prepared in this notebook, see `KEY CELL 02`.\n",
    "- Run all the following cells in the notebook and note the in accuracy metrics for the selected pipeline.\n",
    "- Optionally, create your own pipeline and experiment with that to further improve the  accuracy metrics of your solution.\n",
    "Note: Changing the ingestion should change the accuracy of the retriever. Exactly how much will depend on your content. \n",
    "The two example pipelines in this notebook may not make much of a difference for your content. \n",
    "If you have time, you will learn most by experimenting with creating you own and seeing the change in the metrics.\n",
    "\n",
    "Wrapping up:\n",
    "- For each pipeline that you experiment with, copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell, along with the definition of the pipeline being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which pipeline was best, did you see a major improvement in the metrics, can you suggest a reason for why the best pipeline is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-04-my-data-pipeline.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fec15-e26a-4764-ba7e-db8e50d43d81",
   "metadata": {
    "tags": []
   },
   "source": [
    "**05** Using your data experiment with different values of k\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-04-my-data-pipeline.ipynb`, and paste it into the same directory.\n",
    "- Make sure the the notebook is configured to use the best performing pipeline and embeddings model.\n",
    "- Change the value of *k* that is going to be used from the original setting. \n",
    "There are two other options prepared in this notebook, see `KEY CELL 03`.\n",
    "- Run all the cells that follow below in the notebook and note the in accuracy metrics for the selected value of *k*. \n",
    "*Note*: with this setting, you do not need to re-run the cells above the cell where you make the update.\n",
    "\n",
    "\n",
    "Wrapping up:\n",
    "- For each value of *k* set for the `retriever`, run the evaluation and copy the Top-level Evaluation Results into this cell, along with noting the value of *k* being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which value of *k* gave the best results, did you see a major improvement in the metrics, can you suggest a reason for why the best pipeline is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-05-my-data-pipeline-with-k.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9be75-ae92-480b-b769-0432f1e2c87e",
   "metadata": {},
   "source": [
    "**06** Summarize\n",
    "\n",
    "- Breifly summarize, in four paragraphs what your learned, regarding the following topics:\n",
    "    - The configuration of the retriever for your content\n",
    "    - The process of experimentating with different settings and evaluating the results\n",
    "    - Which evaluation metric was most useful and why\n",
    "    - What might you do next, if you had a 40 hours or more to work on this, to further improve the quality of the retriever\n",
    "- Save your summary in this cell.\n",
    "    \n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-06-summary.ipynb`. Download the notebook for assignment submission.\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
