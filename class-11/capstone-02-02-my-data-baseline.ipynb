{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ede245-8bd8-4ca3-8922-c3b725ba05ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AISA Capstone 2 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc35067-b7cf-438c-81ff-1eb4ed6a9725",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook provides an environment for you to build your intuition on the steps to take when developing a high quality Retrieval Augmented Generation (RAG) solution. \n",
    "RAG solutions retrieve data before calling the large language model (LLM) to generate an answer. \n",
    "The retrieved data is used to augment the prompt to the LLM by adding the relevant retrieved data in context. \n",
    "Any RAG solution is only as good as the quality of the data retrieval process, and this is the particular focus of this notebook, and the accompanying questions and tasks.\n",
    "\n",
    "The RAG solution developed here is enabled by the Llamaindex framework. This is a popular framework in the industry for developing RAG and Agent based solutions. In addition to providing a core set of tools for orchestration of RAG and Agent workflows, there is broad integration with a variety of platforms for model inference (LLM, embedding, ...), and, importantly, tooling for solution evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4d6f9-7cc1-416b-b035-8403ca7bc8f0",
   "metadata": {},
   "source": [
    "## Prerequisites for running the notebook\n",
    "- That you have granted access to the Bedrock models that you are going to use, in the region (**us-west-2**) where you are going to use Bedrock - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html)\n",
    "- Your SageMakerExecutionRole has permissions to invoke Bedrock models - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-prereq.html)\n",
    "- This notebook has been tested with SageMaker Notebook Instance running a `conda_python3` kernel\n",
    "- The AWS region set for Amazon Bedrock use, needs to be in a region where the models being used are 1/ available, and 2/ enabled for use. This notebook was tested with Bedrock region `us-west-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514f985-a013-48e9-acd9-73d4eeb137f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementation\n",
    "This notebook uses llamaindex to define and execute the RAG solution. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: e.g. Anthropic Claude Haiku available through Amazon Bedrock\n",
    "\n",
    "  LLMs are used in the notebook for 1/ RAG response generation, to show the overall RAG workflow in actions, and 2/ for generating test questions on the indexed content (llamaindex nodes) for retrieval evaluation.\n",
    "  \n",
    "- **Text Embeddings Model**: e.g. Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This embedding model is used to generate semantic vector representations of the content (llamaindex nodes) to be stored and the questions input to the RAG solution.\n",
    "  \n",
    "- **Document Loader**: SimpleDirectoryReader (Llamaindex)\n",
    "\n",
    "  Before your chosen LLM can act on your data you need to load it. The way LlamaIndex does this is via data connectors, also called 'Reader'. Data connectors ingest data from different data sources and format the data into Document objects. A Document is a collection of data (currently text, and in future, images and audio) and metadata about that data.\n",
    "  \n",
    "  This implementation use SimpleDirectoryReader, which creates documents out of every file in a given directory. It can read a variety of formats including Markdown, PDFs, Word documents, and PowerPoint decks.\n",
    "\n",
    "- **Vector Store**: VectorIndex (Llamaindex)\n",
    "\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "  \n",
    "  LlamaIndex abstracts the underlying vector database storage implementation with a VectorIndex class. This warps the Index, which is a data structure composed of Document objects, designed to enable querying by an LLM. The Index is designed to be complementary to your querying strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe5196-2edf-4f54-be53-12bc4d70e3ea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da34a35-29ee-4c26-8398-7e1bfe7c58f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293136b-753b-4774-95bc-cd965a38b13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install required Python modules for constructing the RAG solution.\n",
    "You only need to run this once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c358c2-f00d-4582-867a-1ae49bfa9354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.12.19)\n",
      "Requirement already satisfied: llama-index-llms-bedrock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: llama-index-embeddings-bedrock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.19 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.12.19)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.6.7)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.3.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.4.5)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-llms-bedrock) (1.36.1)\n",
      "Requirement already satisfied: llama-index-llms-anthropic<0.7.0,>=0.6.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-llms-bedrock) (0.6.5)\n",
      "Requirement already satisfied: aioboto3<14.0.0,>=13.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-embeddings-bedrock) (13.4.0)\n",
      "Requirement already satisfied: aiobotocore==2.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (2.18.0)\n",
      "Requirement already satisfied: aiofiles>=23.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (24.1.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (3.11.11)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.36.2,>=1.36.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.36.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (2.9.0.post0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (6.1.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (2.3.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.17.2)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0.0,>=1.34.26->llama-index-llms-bedrock) (0.11.2)\n",
      "Requirement already satisfied: openai>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.63.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.0.37)\n",
      "Requirement already satisfied: dataclasses-json in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2025.2.0)\n",
      "Requirement already satisfied: httpx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-core<0.13.0,>=0.12.19->llama-index) (0.9.0)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
      "Requirement already satisfied: anthropic>=0.41.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (0.46.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.5.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.18.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.41.0->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.41.0->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.41.0->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic>=0.41.0->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (1.3.1)\n",
      "Requirement already satisfied: google-auth<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (2.38.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.19->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.26.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic>=0.41.0->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (1.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth<3,>=2->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth<3,>=2->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-auth<3,>=2->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (4.7.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3<14.0.0,>=13.1.1->llama-index-embeddings-bedrock) (1.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.19->llama-index) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=2->anthropic[bedrock,vertex]>=0.41.0->llama-index-llms-anthropic<0.7.0,>=0.6.3->llama-index-llms-bedrock) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \\\n",
    "    llama-index \\\n",
    "    llama-index-llms-bedrock \\\n",
    "    llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724a2d-6686-4177-baa1-3fed868170d5",
   "metadata": {},
   "source": [
    "Download the default RAG test source data to our target source_docs directory. \n",
    "You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5190bed5-26d3-4897-a270-74087c9af1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_docs_dir = './source_docs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d14fc8-2f54-4a24-9ad4-9e84678e9495",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following creates the source_docs directory and downloads a document to that directory. The contents of this directory, \n",
    "initially the document that is downloaded here, will be used in the steps that follow.\n",
    "\n",
    "After running this notebook in its entirity and reviewing its operation, delete this content and add your own content to the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886e9c7c-3e23-40ba-a7c1-b852d083aa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-19 15:07:15--  https://www.buhurtinternational.com/_files/ugd/d219c5_5623c98ecdd142c8a7221c2d00cb621a.pdf\n",
      "Resolving www.buhurtinternational.com (www.buhurtinternational.com)... 34.149.87.45\n",
      "Connecting to www.buhurtinternational.com (www.buhurtinternational.com)|34.149.87.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12555242 (12M) [application/pdf]\n",
      "Saving to: ‘./source_docs//buhurt_armor_requirement.pdf’\n",
      "\n",
      "100%[======================================>] 12,555,242  61.7MB/s   in 0.2s   \n",
      "\n",
      "2025-02-19 15:07:15 (61.7 MB/s) - ‘./source_docs//buhurt_armor_requirement.pdf’ saved [12555242/12555242]\n",
      "\n",
      "--2025-02-19 15:07:15--  https://www.buhurtinternational.com/_files/ugd/d219c5_afd49fe587524088b3136c910bb2ca51.pdf\n",
      "Resolving www.buhurtinternational.com (www.buhurtinternational.com)... 34.149.87.45\n",
      "Connecting to www.buhurtinternational.com (www.buhurtinternational.com)|34.149.87.45|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 711599 (695K) [application/pdf]\n",
      "Saving to: ‘./source_docs//buhurt_weapon_requirement.pdf’\n",
      "\n",
      "100%[======================================>] 711,599      887KB/s   in 0.8s   \n",
      "\n",
      "2025-02-19 15:07:17 (887 KB/s) - ‘./source_docs//buhurt_weapon_requirement.pdf’ saved [711599/711599]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and load data\n",
    "!mkdir -p {source_docs_dir}\n",
    "!wget --no-check-certificate 'https://www.buhurtinternational.com/_files/ugd/d219c5_5623c98ecdd142c8a7221c2d00cb621a.pdf' -O {source_docs_dir}'/buhurt_armor_requirement.pdf'\n",
    "!wget --no-check-certificate 'https://www.buhurtinternational.com/_files/ugd/d219c5_afd49fe587524088b3136c910bb2ca51.pdf' -O {source_docs_dir}'/buhurt_weapon_requirement.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab5b10-dcb8-423c-b1ef-d39a656f6b41",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import auxilliarty modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f69ef6-0453-465f-adc0-3180b7f41352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3  # AWS SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65d3264-223c-4902-8e21-6886e25a52b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is required when running within a jupyter notebook, otherwise you will get errors when llamaindex modules run\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b618759-983d-4d7e-8bf6-840b355b5609",
   "metadata": {},
   "source": [
    "Import required Python modules for constructing and evaluating the RAG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5025ceaf-9699-43ba-9369-773612652b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    RetrieverEvaluator,\n",
    "    generate_question_context_pairs,\n",
    ")\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Response,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66520-b9b5-4fdc-a00b-45d23734bfdb",
   "metadata": {},
   "source": [
    "## Configure the models that will be used for the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b8643-0fc7-4984-bfb4-820b92ce0e99",
   "metadata": {},
   "source": [
    "**Note**: By default this notebook with use the `us-west-2` region. This region has support for the models used in this notebook. You should not need to change this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c946049d-010b-4485-83ce-08425c254261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AWS_REGION = \"us-west-2\"\n",
    "AWS_REGION = \"us-east-1\"  # this is an alternative setting to use if desired "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6109-6dba-4312-951a-af27765774b3",
   "metadata": {},
   "source": [
    "Define the set of Bedrock model IDs that we that we'll use when developing and testing our solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8d3b8-0a47-45db-9b2f-3265d0c795f0",
   "metadata": {},
   "source": [
    "Establish a connection to the Amazon Bedrock service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614ca8a1-d4b7-463e-8770-76bf83a37246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d038da-c7fb-4c07-957c-c23a1effe13e",
   "metadata": {},
   "source": [
    "### Configure the target embeddings models for use with Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf2a047-82b9-4ee9-bbcc-4ab33d9a07f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_multilingual_v1_id = \"amazon.titan-embed-text-v1\"\n",
    "titan_text_embeddings_multilingual_v2_id = \"amazon.titan-embed-text-v2:0\"\n",
    "cohere_text_embeddings_english_id = \"cohere.embed-english-v3\"\n",
    "cohere_text_embeddings_multilingual_id = \"cohere.embed-multilingual-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ca6ba-721d-4af6-a25f-0026218373c5",
   "metadata": {},
   "source": [
    "Configure our chosen embeddings model for use with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac101f7-1f62-4720-9a94-64bdce9102de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_v2 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v2_id,region_name=AWS_REGION)\n",
    "titan_text_embeddings_v1 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v1_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_english = BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_multilingual= BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac963474-ee9e-47f4-ae7f-5896b3ee805d",
   "metadata": {},
   "source": [
    "### Configure the target LLMs for use with Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66fcc-a3c2-4acd-9004-53d96c083391",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following Mistral models produce good questions for evaluation. The Titan model produces questions of lesser quality \n",
    "and sometimes not in the format needed by the tools. \n",
    "\n",
    "**Note** Most Bedrock LLMs do note produce questions in a format that can be directly used for evaluation with the tooling as it is configured in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d60343-0fad-4fad-a6a0-1a28a60c5539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id=\"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "titan_text_express_id = \"amazon.titan-text-express-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad66c9c9-752f-425c-b783-c5951c47d95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the parameters to be applied when invoking the model\n",
    "model_kwargs_llm = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 200,\n",
    "    \"max_tokens\": 4096\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7223cc97-dc5b-40f4-8acb-92be61e03885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_mistral7b = Bedrock(model=instruct_mistral7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_mixtral8x7b = Bedrock(model=instruct_mixtral8x7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_titan_express = Bedrock(model=titan_text_express_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babd1c3-cc16-4914-ba7a-83185c56a418",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the embeddings model to use for the cells that follow\n",
    "\n",
    "The embeddings model is a critical choice for the accuracy of your RAG solution.\n",
    "Experiment with the options here to see which is best for your content.\n",
    "If you want more, test with further alternatives. There are many that are readily supported by llama_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52d75b4-f9ef-4398-9214-15920b917ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL 01\n",
    "\n",
    "embed_model = titan_text_embeddings_v1\n",
    "# embed_model = titan_text_embeddings_v2\n",
    "# embed_model = cohere_text_embeddings_english\n",
    "# embed_model = cohere_text_embeddings_multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d0f7-6744-41e8-a62b-8f5624fe54f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the LLM to use for the cells that follow\n",
    "The LLM will be used for question generation and RAG answer generation in this notebook as it is currently configured.\n",
    "The default value llm_mistral7b works well with the code and should be used if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82ea65f7-1428-41c0-850d-c1eb176c41e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = llm_mistral7b\n",
    "# llm_model = llm_mixtral8x7b\n",
    "# llm_model = llm_titan_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58bbe67d-5908-4d3f-8c0a-831391a4ad5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set LlamaIndex default model settings to what was set in the cells above\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46229098-ce14-4834-a863-2035547a53ab",
   "metadata": {},
   "source": [
    "## Read in the documents for adding to our data store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fc1fd-f34b-41a1-9d88-e4647a521273",
   "metadata": {},
   "source": [
    "Read in the documents in the 'data/source_docs' directory into a structure ready for use by llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486bd6d1-2454-45ff-a33e-fcbf3ea77867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(source_docs_dir)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b952f0-8f8a-4920-ad35-e8abe08fc429",
   "metadata": {},
   "source": [
    "Quick check here to see that all of your documents were read. The count should match the number of pages in the documents in source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "752782ee-1d5c-41d7-8266-c1a6c9194a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261170a-c9db-4f59-b391-9e5039397e26",
   "metadata": {},
   "source": [
    "## Create and run the document ingestion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1fda-bc75-42a1-be1b-54c29a624abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell defines two different document ingestion pipelines. \n",
    "If you have time, test using both of these, and create you own and test with that also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773aa337-a276-441a-92be-190a2d9f37e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define two transformation for the ingestion pipelines for initial experimentation\n",
    "\n",
    "transformations_00=[\n",
    "        TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=100),\n",
    "        embed_model,\n",
    "    ]\n",
    "\n",
    "transformations_01=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=100),\n",
    "        TitleExtractor(),\n",
    "        embed_model,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b013b-f07e-441d-8396-dab15325e7d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the data ingestion pipeline for processing the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7134f8fb-03eb-4167-9063-4fcd08ed7643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL 02\n",
    "\n",
    "# create the pipeline with one of the transformation configurations defined above\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations_00)\n",
    "# pipeline = IngestionPipeline(transformations=transformations_01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314c658-de88-432b-98bc-08a5ccf7bee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the configured ingestion pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96c6cf10-0dcc-40eb-a659-3b70cfb4a939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 26\n"
     ]
    }
   ],
   "source": [
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(f\"number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331bd97-75af-432b-a77c-a82be092585d",
   "metadata": {
    "tags": []
   },
   "source": [
    "This may make test analysis easier. It is none essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b464f0b-b123-4a12-b85f-8718fcaee4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default, the node ids are set to random uuids.\n",
    "# To ensure same id's per run, we manually set them to consistent sequential numbers.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cffb0df-bf84-4630-87b0-ba8fae23f461",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.240234375, 0.298828125, 0.3671875, -0.51171875, -0.2041015625, -0.0184326171875, -0.0126953125, 0.0004367828369140625, -0.1748046875, -0.2333984375, 0.07861328125, 0.23046875, -0.2421875, 0.2431640625, 0.06884765625, 0.12890625, 0.26953125, 0.66015625, -0.08935546875, 0.3359375, -0.0096435546875, 0.1923828125, -0.08203125, 0.5390625, -0.05712890625, -0.51171875, 0.1083984375, 0.004180908203125, -0.03759765625, 0.068359375, -0.671875, -0.134765625, -0.11181640625, -0.43359375, -0.032958984375, -0.08056640625, 0.3828125, 0.0361328125, 0.31640625, 0.0859375, 0.0341796875, 0.2890625, 0.1416015625, -0.8125, -0.306640625, -0.283203125, 0.115234375, 0.083984375, 0.2119140625, 0.2353515625, 0.1416015625, 0.380859375, 0.47265625, 0.1025390625, 0.057373046875, 0.515625, 0.263671875, 0.212890625, -0.2890625, -0.24609375, 0.271484375, 0.0125732421875, 0.033447265625, 0.466796875, 0.061767578125, 0.09619140625, 0.486328125, -0.365234375, 0.49609375, 0.373046875, 0.267578125, -0.13671875, 0.1865234375, 0.25390625, 0.02587890625, 0.060791015625, -0.271484375, 0.0159912109375, -0.2275390625, -0.443359375, -0.375, 0.1728515625, -0.1435546875, 0.48046875, 0.140625, 0.408203125, 0.1630859375, 0.140625, 0.00013637542724609375, -0.003021240234375, -0.12890625, 0.310546875, -0.140625, 0.412109375, 0.18359375, 0.01116943359375, -0.447265625, 0.10693359375, -0.140625, -0.5546875, 0.1982421875, -0.0478515625, -0.169921875, -0.259765625, 0.1015625, 0.0361328125, 0.330078125, -0.26171875, 0.0693359375, 0.443359375, 0.388671875, -0.3671875, 0.04150390625, -0.0238037109375, 0.2001953125, -0.078125, 0.037841796875, 0.0021514892578125, -0.82421875, -0.267578125, -0.330078125, 0.33203125, -0.267578125, 0.0274658203125, -0.1494140625, -0.058349609375, -0.050537109375, 0.01519775390625, 0.1083984375, -0.1513671875, 0.55078125, 0.0654296875, -0.431640625, -0.1669921875, 0.271484375, -0.045654296875, 0.328125, -0.2578125, 0.41015625, 0.23046875, -0.2412109375, -0.306640625, -0.0693359375, -0.439453125, 0.189453125, 0.11083984375, 0.051025390625, 0.19921875, -0.828125, -0.60546875, -0.439453125, -0.26953125, -0.203125, 0.5625, -0.029541015625, -0.0322265625, -0.1630859375, 0.181640625, -0.9375, 0.111328125, 0.055419921875, 0.08935546875, 0.11328125, 0.05029296875, -0.11865234375, 0.055908203125, 0.10498046875, 0.033935546875, -0.169921875, -0.033447265625, 0.083984375, -0.49609375, 0.078125, -0.05908203125, 0.1416015625, 0.00262451171875, 0.039306640625, -0.2890625, 0.038818359375, 0.373046875, 0.06298828125, -0.03955078125, -0.1923828125, -0.458984375, 0.2353515625, 0.1884765625, -0.11376953125, 0.357421875, 0.390625, -0.07080078125, 0.1904296875, -0.2216796875, 0.1806640625, -0.388671875, 0.39453125, 0.267578125, 0.1455078125, 0.11865234375, -0.19921875, 0.302734375, -0.07763671875, 0.216796875, 0.3203125, -0.12890625, 0.298828125, 0.34765625, -0.13671875, 0.212890625, 0.4765625, -0.06982421875, -0.169921875, -0.302734375, -0.05029296875, -0.248046875, 0.126953125, 0.69921875, 0.2353515625, 0.126953125, -0.20703125, 0.267578125, 0.625, 0.380859375, -0.80859375, -0.310546875, -0.1982421875, -0.447265625, 0.6640625, -0.15625, 0.283203125, -0.23046875, 0.263671875, 0.1826171875, 0.30078125, 0.28125, -0.2060546875, -0.064453125, -0.216796875, 0.052734375, -0.2080078125, 0.1044921875, 0.287109375, -0.310546875, -0.58203125, -0.2197265625, -0.45703125, 0.70703125, 0.1201171875, 0.52734375, -0.07763671875, -0.341796875, 0.2373046875, 0.07568359375, -0.00933837890625, 0.1923828125, -0.296875, -0.1806640625, -0.53125, -0.4296875, 0.134765625, 0.068359375, 0.1552734375, -0.4765625, 0.1123046875, -0.11962890625, -0.69140625, -0.25, 0.123046875, 0.244140625, 0.00433349609375, 0.1318359375, 0.00012302398681640625, 0.251953125, -0.01470947265625, -0.46484375, -0.146484375, 0.0498046875, 0.0272216796875, 0.07177734375, 0.181640625, 0.2041015625, 0.11474609375, -0.177734375, 0.061767578125, 0.154296875, -0.2080078125, -0.28125, 0.384765625, -0.427734375, 0.7109375, 0.05322265625, -0.357421875, -0.392578125, -0.01165771484375, 0.30859375, -0.2080078125, -0.1103515625, -0.035888671875, -0.0269775390625, -0.06982421875, -0.0595703125, 0.17578125, -0.23828125, 0.1767578125, -0.1875, 0.04296875, 0.07275390625, 0.00823974609375, -0.2197265625, -0.0244140625, 0.1953125, -0.5703125, 0.1357421875, 0.302734375, 0.443359375, -0.25, 0.1171875, -0.0322265625, 0.13671875, -0.4921875, -0.263671875, -0.21484375, -0.330078125, 0.283203125, -0.2158203125, -0.034912109375, 0.07177734375, 0.06298828125, -0.2470703125, 0.3203125, 0.400390625, -0.25, -0.404296875, 0.60546875, -0.0537109375, 0.048095703125, -0.146484375, -0.11767578125, -0.46875, -0.515625, 0.2080078125, -0.08935546875, 0.068359375, -0.40234375, 0.28125, 0.10498046875, -0.337890625, -0.1279296875, -0.1533203125, -0.1630859375, 0.130859375, -0.10205078125, -0.1767578125, 0.306640625, 0.076171875, 0.255859375, 0.055419921875, -0.1572265625, 0.0478515625, -0.83984375, -0.017578125, 0.126953125, 0.1982421875, 0.142578125, -0.07666015625, 0.083984375, 0.044921875, -0.1640625, 0.41796875, 0.275390625, -0.265625, -0.19921875, 0.85546875, 0.158203125, -0.07421875, 0.119140625, -0.005859375, -0.232421875, -0.1064453125, -0.1611328125, 0.65625, -0.671875, 0.10400390625, -0.314453125, -0.2353515625, 0.01458740234375, -0.0654296875, -0.33984375, 0.26953125, -0.1650390625, 0.390625, -0.0693359375, 0.07470703125, -0.09375, -0.4453125, 0.189453125, -0.2373046875, 0.25, -0.08349609375, -0.52734375, -0.10546875, -0.1171875, 0.1640625, -0.2392578125, -0.18359375, 0.06201171875, 0.53515625, 0.388671875, -0.62109375, 0.390625, -0.20703125, -0.138671875, 0.11083984375, 0.12158203125, 0.018798828125, 0.10107421875, -0.03662109375, -0.1923828125, -0.11962890625, 0.244140625, 0.267578125, 0.19140625, 0.10546875, 0.0791015625, -0.03369140625, 0.2275390625, 0.310546875, 0.04296875, -0.2470703125, -0.1748046875, 0.275390625, -0.458984375, 0.01904296875, -0.08251953125, -0.1923828125, -0.01019287109375, -0.396484375, 0.384765625, -0.345703125, 0.06396484375, 0.111328125, 0.48046875, 0.220703125, 0.3125, 0.08349609375, 0.04931640625, -0.115234375, 0.287109375, 0.341796875, -0.16015625, -0.3125, 0.5625, 0.46875, 0.1845703125, 0.6015625, -0.146484375, -0.458984375, 0.1865234375, -0.17578125, -0.01434326171875, 0.265625, -0.1787109375, -0.1923828125, 0.08984375, -0.57421875, 0.08642578125, -0.130859375, 0.482421875, -0.1865234375, 0.287109375, 0.03271484375, -0.443359375, 0.0791015625, -0.111328125, -0.421875, 0.00091552734375, 0.296875, 0.220703125, 0.0634765625, 0.3671875, -0.125, -0.031982421875, -0.06396484375, -0.5703125, -0.3828125, 0.0018768310546875, 0.421875, 0.765625, -0.0223388671875, -0.279296875, 0.408203125, 0.1357421875, 0.054931640625, -0.2236328125, -0.09619140625, 0.1787109375, 0.345703125, -0.0003757476806640625, -0.052001953125, 0.1748046875, 0.056640625, -0.5078125, -0.373046875, -0.205078125, 0.48828125, -0.1142578125, -0.1640625, 0.384765625, 0.263671875, -0.375, -0.3359375, 0.2333984375, -0.007476806640625, -0.031982421875, 0.046630859375, -0.1484375, 0.2265625, -0.1513671875, 0.0272216796875, -0.37109375, -0.3046875, 0.2158203125, -0.404296875, -0.08984375, -0.056884765625, 0.17578125, 0.021240234375, 0.203125, -0.15625, -0.3359375, -0.408203125, -0.158203125, 0.1416015625, -0.35546875, -0.291015625, 0.6171875, 0.056884765625, 0.12890625, 0.007598876953125, -0.6171875, 0.0537109375, -0.119140625, 0.71875, -0.04736328125, 0.32421875, -0.0576171875, 0.27734375, -0.298828125, 0.1865234375, -0.62109375, -0.3671875, -0.0791015625, 0.126953125, -0.51171875, -0.427734375, 0.212890625, 0.150390625, 0.29296875, 0.40625, -0.2197265625, -0.34375, -0.40234375, 0.427734375, -0.1806640625, 0.158203125, -0.01287841796875, -0.1923828125, -0.02099609375, -0.1552734375, -0.09228515625, -0.09619140625, -0.283203125, -0.1572265625, 0.478515625, 0.03466796875, -0.1787109375, 0.11328125, -0.0263671875, 0.11767578125, 0.1318359375, 0.1806640625, 0.2412109375, -0.08837890625, -0.3515625, -0.0311279296875, 0.380859375, -0.0615234375, 0.1484375, 0.115234375, 0.1396484375, -0.462890625, 0.2451171875, 0.72265625, -0.46484375, -0.298828125, 0.2001953125, 0.400390625, -0.0277099609375, 0.130859375, 0.6484375, -0.2890625, -0.2041015625, -0.0791015625, 0.08447265625, -0.0177001953125, -0.30078125, -0.005401611328125, -0.52734375, -0.419921875, 0.263671875, -0.34765625, 0.001190185546875, 0.001251220703125, -0.037353515625, -0.040283203125, -0.36328125, 0.30078125, 0.21875, -0.3671875, -0.1884765625, 0.466796875, 0.185546875, -0.028564453125, -0.1435546875, 0.251953125, 0.275390625, -0.1640625, 0.29296875, 0.390625, 0.26171875, -0.419921875, -0.40234375, 0.60546875, -0.1474609375, -0.181640625, -0.232421875, 0.5078125, 0.8671875, -0.388671875, -0.16015625, -0.58984375, 0.16796875, -0.181640625, 0.185546875, -0.3515625, 0.66796875, 0.470703125, 0.028564453125, 0.2275390625, 0.25, 0.1904296875, -0.169921875, -0.2578125, 0.408203125, -0.265625, -0.1884765625, -0.138671875, -0.119140625, 0.216796875, -0.2138671875, 0.181640625, -0.06494140625, -0.1376953125, 0.2255859375, 0.06787109375, 0.58203125, -0.3828125, -0.04443359375, -0.2890625, -0.78515625, 0.162109375, 0.1376953125, 0.00799560546875, -0.0673828125, 0.2109375, -0.11865234375, -0.92578125, -0.1826171875, -0.0205078125, -0.330078125, -0.212890625, 0.05029296875, 0.12158203125, 0.02978515625, -0.04052734375, -0.1962890625, 0.205078125, 0.6015625, 0.1923828125, -0.091796875, -0.1455078125, -0.06201171875, 0.240234375, -0.05859375, -0.1962890625, 0.040283203125, -0.1220703125, -0.10986328125, 0.5703125, 0.06591796875, -0.0595703125, 0.1689453125, -0.0155029296875, 0.1787109375, -0.14453125, -0.1806640625, 0.06005859375, 0.921875, -0.359375, 0.69921875, -0.162109375, 0.06005859375, -0.109375, 0.055908203125, -0.267578125, 0.15234375, 0.296875, -0.0107421875, 0.043212890625, 0.072265625, 0.193359375, -0.0771484375, -0.208984375, -0.123046875, -0.19140625, 0.11083984375, 0.1865234375, -0.119140625, 0.349609375, -0.2578125, -0.11669921875, -0.431640625, -0.36328125, 0.224609375, 0.47265625, 0.76953125, 0.328125, -0.5390625, 0.072265625, 0.059814453125, -0.150390625, 0.1591796875, -0.0771484375, 0.0272216796875, 0.0556640625, -0.41796875, 0.14453125, -0.376953125, 0.29296875, 0.291015625, -0.2177734375, 0.171875, -0.10498046875, 0.1435546875, -0.416015625, 0.189453125, 0.1376953125, -0.00628662109375, 0.330078125, -0.546875, 0.24609375, 0.25390625, -0.08203125, 0.07861328125, -0.0771484375, 0.050048828125, 0.0162353515625, -0.28125, 0.0167236328125, 0.1591796875, 0.06494140625, -0.0986328125, -0.32421875, -0.0137939453125, 0.177734375, 0.2451171875, -0.13671875, 0.69921875, 0.7109375, -0.1787109375, -0.287109375, -0.1826171875, -0.1494140625, 0.000751495361328125, -0.1533203125, -0.263671875, 0.6328125, 0.0869140625, 0.2333984375, -0.216796875, -0.1650390625, 0.32421875, 0.328125, 0.1552734375, -0.416015625, -0.109375, 0.2353515625, -0.1962890625, -0.388671875, 0.2451171875, 0.390625, -0.12109375, -0.07470703125, -0.134765625, 0.390625, 0.15234375, -0.173828125, 0.59375, 0.61328125, 0.09326171875, -0.1904296875, -0.6484375, -0.44140625, 0.25, 0.71875, 0.384765625, -0.12109375, -0.3046875, -0.12109375, 0.18359375, -0.2158203125, -0.26953125, 0.28125, -0.251953125, -0.35546875, -0.041015625, 0.00141143798828125, 0.15625, 0.130859375, -0.30859375, 0.52734375, 0.029541015625, -0.328125, 0.490234375, -0.455078125, -0.166015625, -0.1123046875, -0.333984375, -0.1416015625, 0.04345703125, 0.22265625, 0.2255859375, -0.240234375, 0.1337890625, 0.021240234375, -0.09765625, -0.330078125, -0.3203125, 0.392578125, 0.263671875, -0.0634765625, -0.099609375, 0.08349609375, 0.2216796875, -0.416015625, -0.04833984375, -0.2197265625, 0.29296875, -0.1181640625, 0.193359375, 0.07666015625, 0.80078125, 0.341796875, 0.6953125, 0.138671875, -0.609375, -0.1123046875, -0.0196533203125, -0.365234375, 0.23828125, 0.0908203125, 0.1455078125, -0.0106201171875, 0.1025390625, 0.23046875, -0.0162353515625, -0.1982421875, -0.058837890625, 0.10986328125, 0.5546875, 0.1416015625, 0.126953125, -0.435546875, 0.5390625, -0.1220703125, -0.1396484375, 0.14453125, 0.80078125, 0.1357421875, -0.1455078125, -0.20703125, -0.06787109375, 0.0478515625, -0.0167236328125, -0.013916015625, -0.11474609375, 0.37109375, 0.09765625, -0.1044921875, 0.107421875, 0.45703125, -0.369140625, -0.173828125, -0.177734375, 0.017822265625, -0.061279296875, 0.33203125, 0.359375, -0.1357421875, 0.345703125, -0.302734375, -0.33203125, -0.1865234375, -0.12353515625, -0.3359375, -0.005645751953125, 0.0810546875, 0.10693359375, -0.08544921875, -0.045654296875, -0.80859375, 0.0810546875, 0.10302734375, 0.205078125, 0.40234375, -0.2294921875, -0.02197265625, -0.087890625, 0.0458984375, 0.0703125, 0.001312255859375, 0.1259765625, -0.189453125, -0.43359375, -0.027099609375, -0.140625, 0.34375, -0.1943359375, 0.62109375, 0.0849609375, 0.16796875, -0.16015625, -0.00021076202392578125, -0.060302734375, -0.361328125, -0.2138671875, -0.2021484375, -0.28515625, 0.59765625, 0.228515625, 0.482421875, 0.060546875, 0.2080078125, -0.279296875, 0.0002727508544921875, -0.10791015625, -0.0299072265625, -0.2255859375, 0.20703125, 0.3984375, -0.08349609375, 0.0140380859375, -0.22265625, -0.0810546875, 0.10498046875, -0.373046875, -0.27734375, -0.578125, 0.171875, -0.10009765625, 0.028564453125, -0.0233154296875, 0.2451171875, 0.1845703125, -0.7421875, 0.2490234375, 0.0908203125, -0.40234375, -0.1640625, 0.31640625, 0.1689453125, -0.039794921875, -0.455078125, -0.1220703125, -0.0201416015625, 0.06640625, 0.1259765625, 0.609375, -0.16015625, 0.095703125, -0.5078125, 0.00567626953125, 0.251953125, -0.181640625, 0.0157470703125, -0.056884765625, 0.50390625, -0.79296875, -0.06298828125, -0.0101318359375, 0.90234375, -0.369140625, -0.130859375, -0.515625, -0.09228515625, -0.11865234375, 0.031982421875, -0.1611328125, -0.1318359375, -0.1962890625, 0.12890625, 0.033935546875, 0.70703125, -0.6484375, -0.1650390625, 0.1259765625, 0.26953125, -0.390625, -0.027099609375, -0.228515625, 0.2158203125, -0.4375, -0.0625, -1.015625, 0.7109375, -0.4140625, -0.2470703125, -0.054443359375, 0.7109375, -0.3046875, 0.224609375, 0.6015625, 0.2314453125, 0.41015625, 0.08935546875, -0.033935546875, 0.003631591796875, 0.1708984375, 0.404296875, -0.0169677734375, 0.203125, 0.4453125, 0.0177001953125, 0.212890625, 0.515625, -0.169921875, -0.1259765625, -0.46875, -0.341796875, 0.0017852783203125, -0.166015625, -0.55078125, -0.4453125, 0.328125, -0.59765625, 0.111328125, -0.453125, -0.2177734375, -0.08740234375, 0.0634765625, 0.10791015625, 0.64453125, -0.02978515625, -0.0810546875, -0.50390625, 0.61328125, 0.49609375, -0.373046875, -0.21875, -0.72265625, -0.1904296875, -0.1826171875, 0.23828125, -0.003326416015625, 0.36328125, 0.07177734375, 0.05517578125, -0.030517578125, -0.0703125, 0.61328125, -0.10791015625, 0.271484375, 0.453125, 0.1689453125, 0.0194091796875, -0.26171875, -0.1591796875, -0.6328125, -0.25390625, 0.005126953125, 0.09521484375, -0.0220947265625, 0.103515625, 0.1884765625, -0.34375, -0.2294921875, -0.1064453125, 0.421875, -0.34375, -0.4296875, 0.03173828125, 0.357421875, -0.046142578125, 0.1328125, -0.765625, -0.1318359375, -0.057861328125, -0.1533203125, -0.3984375, 0.1396484375, -0.2470703125, -0.10302734375, 0.294921875, 0.224609375, 0.08056640625, -0.3671875, -0.0164794921875, 0.00689697265625, 0.197265625, -0.65234375, 0.1708984375, 0.35546875, 0.2216796875, -0.255859375, -0.072265625, 0.1533203125, -0.00677490234375, 0.16015625, 0.30078125, 0.19921875, 0.1669921875, -0.5078125, 0.310546875, -0.38671875, 0.416015625, 0.392578125, 0.134765625, 0.054443359375, -0.00390625, 0.12109375, -0.212890625, -0.2041015625, -0.60546875, 0.080078125, -0.380859375, -0.32421875, -0.79296875, -0.0286865234375, -0.5859375, 0.2490234375, -0.2021484375, -0.00567626953125, 0.439453125, -0.408203125, 0.057861328125, 0.447265625, 0.0849609375, 0.04833984375, -0.06787109375, -0.5546875, 0.57421875, 0.22265625, -0.330078125, 0.75390625, -0.0849609375, -0.61328125, 0.42578125, 0.37890625, 0.03271484375, 0.2080078125, -0.047119140625, 0.64453125, -0.64453125, 0.1572265625, 0.10107421875, -0.5859375, 0.23046875, -0.4140625, -0.134765625, -0.80859375, -0.5546875, -0.10498046875, -0.1875, -0.380859375, -0.07080078125, -0.248046875, -0.026123046875, 0.189453125, 0.023193359375, -0.3046875, -0.051025390625, 0.1162109375, -0.21484375, -0.13671875, -0.400390625, -0.0869140625, 0.17578125, 0.115234375, 0.4296875, -0.13671875, 0.016845703125, 0.09912109375, -0.1865234375, 0.75390625, -0.1552734375, 0.2060546875, -0.63671875, -0.1884765625, -0.08837890625, -0.031982421875, -0.375, -0.248046875, 0.0172119140625, 0.04638671875, 0.43359375, 0.21484375, 0.6875, -0.0303955078125, -0.28515625, -0.59765625, -0.609375, 0.09228515625, -0.234375, 0.64453125, 0.10009765625, -0.061767578125, 0.09130859375, 0.291015625, -0.609375, -0.41796875, -0.1494140625, -0.265625, -0.45703125, 0.047607421875, 0.341796875, 0.01287841796875, 0.1689453125, 0.52734375, -0.255859375, -0.1806640625, -0.56640625, 0.07958984375, -0.062255859375, 0.2294921875, 0.765625, -0.1259765625, -0.1015625, -0.09228515625, -0.388671875, -0.25390625, 0.36328125, 0.50390625, 0.146484375, 0.1279296875, 0.0654296875, 0.474609375, 0.22265625, -0.330078125, -0.1806640625, 0.59765625, -0.482421875, 0.0888671875, 0.259765625, -0.050537109375, 0.380859375, 0.193359375, 0.05029296875, -0.345703125, 0.2001953125, 0.138671875, -0.216796875, 0.2578125, -0.26953125, 0.34375, -0.4296875, -0.306640625, 0.169921875, 0.2109375, 0.2490234375, -0.27734375, 0.2119140625, -0.4375, -0.04443359375, 0.146484375, 0.049072265625, 0.1396484375, -0.1279296875, 0.3359375, -0.0703125, 0.5625, -0.2080078125, 0.059814453125, -0.419921875, -0.12109375, -0.353515625, -0.224609375, 0.35546875, -0.11962890625, -0.173828125, 0.7421875, 0.279296875, 0.02197265625, 0.2216796875, -0.2734375, 0.2080078125, -0.330078125, 0.5078125, -0.022216796875, -0.0673828125, 0.330078125, -0.5234375, 0.357421875, -0.484375, -0.1689453125, -0.294921875, 0.11865234375, -0.2255859375, 0.1962890625, 0.2490234375, 0.08251953125, -0.11474609375, -0.049072265625, 0.357421875, -0.1884765625, -0.09033203125, 0.078125, -0.427734375, 0.2099609375, -0.0810546875, -0.98828125, 0.09912109375, 0.171875, 0.796875, 0.2373046875, 0.5703125, -0.1953125, 0.06298828125, 0.2373046875, -0.48046875, 0.125, 0.1259765625, 0.053466796875, -0.546875, -0.0732421875, 0.05810546875, -0.5390625, -0.08349609375, -0.0859375, 0.1513671875, 0.1552734375, 0.41015625, 0.228515625, -0.369140625, 0.3671875, -0.20703125, 0.06884765625, 0.1357421875, -0.431640625, -0.54296875, 0.66015625, -0.287109375, 0.3203125, 0.326171875, -0.267578125, -0.6796875, 0.421875, 0.1943359375, -0.298828125, 0.11962890625, 0.15234375, 0.142578125, 0.169921875, 0.3515625, 0.1357421875, 0.30078125, 0.50390625, 0.044189453125, 0.1455078125, -0.236328125, 0.42578125, -0.1982421875, -0.1474609375, -0.09619140625, -0.208984375, 0.310546875, 0.470703125, 0.91796875, 0.0047607421875, -0.1923828125, -0.00946044921875, -0.65234375, -0.05419921875, -0.0283203125, 0.11865234375, -0.205078125, -0.08203125, -0.064453125, 0.10595703125, 0.439453125, 0.0267333984375, 0.033203125, -0.0299072265625, -0.033935546875, -0.32421875, 0.11181640625, -0.33984375, 0.40625, -0.400390625, -0.55859375, -0.0693359375, 0.1123046875, -0.37109375, 0.2265625, 0.16015625, 0.1728515625, -0.59375, -0.640625, 0.0016021728515625, -0.2470703125, 0.69921875, 0.3828125, -0.2138671875, 0.09033203125, -0.055419921875, -0.2001953125, -0.306640625, 0.01092529296875, 0.4140625, -0.2060546875, -0.69140625, -0.1337890625, -0.2490234375, -0.498046875, 0.365234375, -0.388671875, 0.310546875, -0.10400390625, 0.115234375, 0.10791015625, 0.1376953125, -0.0341796875, 0.58203125, 0.1328125, 0.185546875, 0.6015625, 0.09765625, -0.06298828125, 0.453125, -0.40234375, 0.029052734375, -0.322265625, 0.1650390625, 0.4140625, 0.38671875, 0.64453125, -0.40234375, -0.79296875, -0.12890625, -0.10107421875, 0.041259765625, -0.294921875, -0.361328125, 0.369140625, -0.1669921875, 0.3046875, -0.2314453125, -0.06396484375, -0.3359375, 0.0247802734375, -0.1162109375, 0.2421875, 0.453125, -0.2138671875, -0.248046875, -0.00083160400390625, 0.0007476806640625, 0.310546875, 0.146484375, 0.091796875, 0.1767578125, 0.027099609375, -0.181640625, -0.625, 0.251953125, 0.01043701171875, -0.123046875, 0.08740234375, 0.251953125, 0.026611328125, -0.01226806640625, 0.39453125, 0.1552734375, 0.294921875, 0.0028533935546875, 0.154296875, -0.384765625, -0.1748046875, -0.0966796875, 0.216796875, -0.330078125, -0.28515625, 0.2109375, 0.12353515625, -0.2294921875, -0.283203125, 0.3984375, -0.126953125, -0.228515625, -0.19921875, -0.09814453125, 0.57421875, 0.5390625, 0.0810546875, 0.1611328125, -0.28125, 0.478515625, 0.1328125, 0.25390625, -0.455078125, -0.033447265625, -0.0033111572265625, 0.7890625, -0.1923828125, -0.18359375, -0.032958984375, 0.412109375, -0.318359375, -0.6953125, 0.298828125, -0.1748046875, 0.087890625, -0.2470703125, 0.408203125, -0.4375, 0.48046875, -0.79296875, -0.046142578125]\n"
     ]
    }
   ],
   "source": [
    "# validate that node has an embedding associated with it\n",
    "for idx, node in enumerate(nodes):\n",
    "    if node.id_ == \"node_0\":\n",
    "        print(node.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620ecd1-c357-49a0-8921-7de63903f870",
   "metadata": {},
   "source": [
    "## Create the VectorIndex \n",
    "This creates our vector database, in memory in this case,  using the nodes that were created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25602778-44d3-4adf-b3e2-68b679feeea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1160-b3e8-445d-8680-fa9d46faea06",
   "metadata": {},
   "source": [
    "## Test that we have a valid starting point for our evaluation\n",
    "We run a quick system test with the defaul llama_index RAG workflow with a question that is relevant to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3006bf-14de-4c74-93dd-3a584bcb5be9",
   "metadata": {},
   "source": [
    "Instantiate a query engine object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e3b4be-5fe1-4789-b7b9-bf17cb6a9f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c163ad-7a8c-40e1-a9f4-8caac25e8dc1",
   "metadata": {},
   "source": [
    "Specify a question that has can be answered by the document(s) that have been ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3a8e95-22ae-45e2-9f9b-d0734d6a4bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"What are the historical consistency requirements of an armor?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479cecf-4725-45f6-be71-7cd5bf4cf915",
   "metadata": {},
   "source": [
    "Run the default RAG pipeline with the example query. This should give a meaningful result. Don't worry if the answer is overly verbose, etc. We'll fix that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0a8af86-4544-4286-be1a-a32b7fe55c66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " An armor must be derived from historical sources and align with authenticity rules documents. It must date between the 14th and 17th centuries, with reproductions of armors predating the 13th century being prohibited for safety considerations. Prohibited features include indications of modern materials or manufacturing techniques such as neon colors, obvious nylon cords, plastic ties, visible welded seams, and heat-induced discoloration. The armor must consist of pieces from the same style, with distinct styles defined as Western European, Slavic Influence, and Eastern Influence. Western European style includes countries such as Great Britain, Ireland, France, Portugal, Spain, Germany, Italy, Norway, Denmark, Sweden, Finland, Austria, Switzerland, Belgium, and the Netherlands. Slavic Influence includes countries such as the Czech Republic, Romania, Hungary, Poland, Slovakia, Slovenia, Croatia, Latvia, Estonia, Moldova, Serbia, Ukraine, Russia, and Belarus.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(example_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06687f-c443-4bdb-8e70-25686fbd3564",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127b15d-cb49-43ab-8f8e-a908915eb6c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the retrieval accuracy of the VectorIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85730c7c-81f2-493c-8fe0-3a59204e2b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a set of question and node (context) pairs to drive the tests that follow\n",
    "This uses the llm that give the methods and the document data stored in the nodes (created during document ingestion)\n",
    "\n",
    "This will make many calls to the specified LLM (num_questions_per_chunk * number of nodes). This will likely be throttled by Bedrock. The llama_index API will work through the throttling except in extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3a0e486-180b-42e3-8ca3-852e73690c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 4/26 [00:06<00:39,  1.79s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 23%|██▎       | 6/26 [00:29<02:17,  6.86s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 31%|███       | 8/26 [00:51<02:40,  8.94s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 35%|███▍      | 9/26 [01:09<03:21, 11.88s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      "Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 42%|████▏     | 11/26 [01:38<03:10, 12.67s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 54%|█████▍    | 14/26 [02:02<01:44,  8.68s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      "Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 62%|██████▏   | 16/26 [02:41<02:15, 13.58s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 65%|██████▌   | 17/26 [02:57<02:09, 14.44s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 77%|███████▋  | 20/26 [03:30<01:10, 11.76s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 81%|████████  | 21/26 [03:45<01:03, 12.75s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      " 96%|█████████▌| 25/26 [04:30<00:10, 10.92s/it]Retrying llama_index.llms.bedrock.utils.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ThrottlingException: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again. You have sent too many requests.  Wait before trying again..\n",
      "100%|██████████| 26/26 [04:42<00:00, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 ms, sys: 23.6 ms, total: 449 ms\n",
      "Wall time: 4min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_dataset = generate_question_context_pairs(nodes, num_questions_per_chunk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9794d-4890-42ca-867b-5558955932e9",
   "metadata": {},
   "source": [
    "Take a look at the sample queries generated. This should show a meaningful questions related to your document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d29a7fc-22b4-4ff4-98d6-764280755a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context information given, here is a question that could be asked for an upcoming quiz or examination:\n",
      "What are the historical consistency requirements for armor equipment according to the given document?\n",
      "In what categories does the document outline the use of armors, shields, and weapons in combat?\n",
      "Which historical period does the 14th century Western European style of armor belong to?\n",
      "Which email address should competitors contact to obtain validation for deviations from the specified armor styles as outlined in the technical requirements document?\n",
      "What are the two main responsibilities of every competitor regarding their equipment in full contact fights according to the given context?\n",
      "What are the minimum requirements for a helmet as stated in the context document?\n",
      "What material is recommended for the dome and visor of a helmet according to the given document?\n",
      "What is the minimum recommended thickness for mild steel plates used in armor?\n",
      "What type of gauntlets cannot be opened and are sometimes referred to as \"torpedo,\" \"hoof,\" or \"fist\" gauntlets, excluding those used in Longsword Duels?\n",
      "Which type of armor is required for protecting joints according to the given document?\n",
      "Based on the changelog, when was the document first created?\n",
      "Based on the context information given, here is a question that could be asked for an upcoming quiz or examination:\n",
      "What are the different categories of weapons listed in the table of contents?\n",
      "What is the definition of equipment as mentioned in the document?\n",
      "Which time period does the 14th century Western European style armor belong to, according to the given context?\n",
      "Which time period does the Eastern influence in art and culture span, as specified in the document?\n",
      "What is every competitor's responsibility regarding the quality and safety of their equipment according to the given context?\n",
      "What are the two main types of weapons according to the context information?\n",
      "What are the safety requirements for the radius and thickness of the edges of bladed weapons in a tournament?\n",
      "What are the two required methods for securing the head of a hafted weapon to the haft according to the given specifications?\n",
      "What type of material is strictly prohibited for making the haft of a weapon according to the given guidelines?\n",
      "What is the minimum distance from the top edge of the handle for outrance bladed weapons in women's fights?\n",
      "Which weapon categories are permitted for duels according to the given document?\n",
      "What type of weapons are allowed for use in Buhurt/Group combat based on the given document?\n",
      "Based on the changelog, what was added to the document in December 2024 regarding weapons?\n"
     ]
    }
   ],
   "source": [
    "for item in list(qa_dataset.queries.items()):\n",
    "    print(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a4af-f49d-4eea-82ca-e7d5cca9b990",
   "metadata": {},
   "source": [
    "## Instantiate a retriever against the index for testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fad7c-f10c-46a3-8a9a-6d3e5c25b64d",
   "metadata": {},
   "source": [
    "### Set the number of items to return from the Retriever\n",
    "This is a trade-off item, more returned content is not always better. Consider how this may impact your pipeline and evaluation results and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d66407c-ea89-4c90-8be0-1bc70c2700fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL: 03\n",
    "\n",
    "number_of_items_to_return = 2\n",
    "# number_of_items_to_return = 3\n",
    "# number_of_items_to_return = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d6db691-6af4-4856-8ad2-604c0c6f629c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=number_of_items_to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab82df-ebaa-4a2f-a98b-298198672f26",
   "metadata": {},
   "source": [
    "Run a quick system test on the retriever and check that the output nodes look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5e6327e-46ca-4e1d-9979-11c86471d784",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='node_3', embedding=None, metadata={'page_label': '4', 'file_name': 'buhurt_armor_requirement.pdf', 'file_path': '/home/ec2-user/SageMaker/elvtr-ai-solution-architect/class-11/source_docs/buhurt_armor_requirement.pdf', 'file_type': 'application/pdf', 'file_size': 12555242, 'creation_date': '2025-02-19', 'last_modified_date': '2024-12-11'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5d80a8a0-7d8b-4e57-9470-9a4255bd619e', node_type='4', metadata={'page_label': '4', 'file_name': 'buhurt_armor_requirement.pdf', 'file_path': '/home/ec2-user/SageMaker/elvtr-ai-solution-architect/class-11/source_docs/buhurt_armor_requirement.pdf', 'file_type': 'application/pdf', 'file_size': 12555242, 'creation_date': '2025-02-19', 'last_modified_date': '2024-12-11'}, hash='b5e8d209574250c98800099c9321346a2374e4aa1e3608fc3f5b2f619cd6fa8a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2. Historical ConsistencyRequirements\\n2.1.Equipmentfromhistoricalsources● Onlyarmorsderivedfromhistorical sourcesarepermittedforuse.● Armorsmust alignwithAuthenticityRulesdocuments.\\n2.2.Datesofsources● Armorsmust alignwithsourcesdatingbetweenthe14th(1300)and17th(1600)centuries.● For safety considerations, reproductions of armors predating the 13th century areprohibited.\\n2.3.Prohibitedfeatures● Prohibited features encompass evident indications of modern materials or manufacturingtechniques including: neoncolors, obvious nyloncords, plastic ties, visibleweldedseams,heat-induceddiscoloration, modernfootwearandothervisiblemodernequipment.\\n2.4.ConsistencyinEquipmentArmors, shields and weapons must consist of pieces fromthesamestyle. Distinct styles aredefinedinmodern-daytermsas:\\n2.4.1.WesternEuropestyle● 14thcentury:from1300to1380● Transitional:from1380to1420● 15th century: from1420 to 1500. XVth stylearmor must beapprovedby theAuthenticity Committee. We recommended seeking approval before buyingsucharmor.● Western Europe includes the following modern countries: Great Britain,Ireland, France, Portugal, Spain, Germany, Italy, Norway, Denmark, Sweden,Finland, Austria, Switzerland, Belgium, andtheNetherlands2.4.2.SlavicInfluence● Central Europe14th:from1300to1400● Russianlatearmors:from1500to1700● SlavicInfluenceincludesthefollowingmoderncountries:CzechRepublic,Romania, Hungary, Poland, Slovakia, Slovenia, Croatia, Latvia, Estonia,Moldova, Serbia, Ukraine, Russia, andBelarus.2.4.3.Easterninfluence● Chinesestyle: from1300to1600● Japanesesamurai style: from1400to1700● Middle-East style: from1300to1700● Easterninfluenceincludes thefollowingmoderncountries: China, Japan, India,Korea, Iran, Iraq, Turkey, Egypt\\nTechnicalRequirementsforarmors,weaponsandshields 3', mimetype='text/plain', start_char_idx=0, end_char_idx=1777, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7229607406768299), NodeWithScore(node=TextNode(id_='node_15', embedding=None, metadata={'page_label': '4', 'file_name': 'buhurt_weapon_requirement.pdf', 'file_path': '/home/ec2-user/SageMaker/elvtr-ai-solution-architect/class-11/source_docs/buhurt_weapon_requirement.pdf', 'file_type': 'application/pdf', 'file_size': 711599, 'creation_date': '2025-02-19', 'last_modified_date': '2024-12-14'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6a8fd583-1eca-48e9-b17b-60742765b2a9', node_type='4', metadata={'page_label': '4', 'file_name': 'buhurt_weapon_requirement.pdf', 'file_path': '/home/ec2-user/SageMaker/elvtr-ai-solution-architect/class-11/source_docs/buhurt_weapon_requirement.pdf', 'file_type': 'application/pdf', 'file_size': 711599, 'creation_date': '2025-02-19', 'last_modified_date': '2024-12-14'}, hash='3b1d22f2a0a4b2514d9ded351ada1f0bb0beb246c59cc7e291592d9d163a6742')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2.Historical ConsistencyRequirements\\n2.1.Equipmentfromhistorical sources● Onlyweaponsderivedfromhistorical sourcesarepermittedforuse.\\n● Weaponsmust alignwithAuthenticityRulesdocuments.\\n2.2.Datesofsources● Weapons must align with sources dating between the 14th (1300) and 17th (1600)centuries.\\n● For safety considerations, reproductions of armors predating the 13th century areprohibited, asareweaponsof thesameperiod.\\n2.3.Prohibitedfeatures● Prohibited features encompass evident indications of modern materials or manufacturingtechniques including: neoncolors, obvious nyloncords, plastic ties, visibleweldedseams,heat-induceddiscolorationandothervisiblemodernequipment.\\n2.4.ConsistencyinEquipmentArmors, shields and weapons must consist of pieces from the same style. Distinct styles aredefinedinmodern-daytermsas:\\n2.4.1. WesternEuropestyle● 14thcentury: from1300to1380\\n● Transitional: from1380to1420\\n● 15th century: from1420 to 1500. XVth style armor must be approvedby theAuthenticityCommittee. Werecommendedseekingapproval beforebuyingsucharmor.\\n● Western Europe includes the following modern countries: Great Britain, Ireland, France,Portugal, Spain, Germany, Italy, Norway, Denmark, Sweden, Finland, Austria, Switzerland,Belgium, andtheNetherlands\\n2.4.2. SlavicInfluence● Central Europe14th: from1300to1400\\n● Russianlatearmors: from1500to1700\\n● Slavic Influence includes the following modern countries: Czech Republic, Romania,Hungary, Poland, Slovakia, Slovenia, Croatia, Latvia, Estonia, Moldova, Serbia, Ukraine,Russia, andBelarus.', mimetype='text/plain', start_char_idx=0, end_char_idx=1542, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6443958230009412)]\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = retriever.retrieve(example_query)\n",
    "print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f47e2-1e69-4a8e-a26c-886ef2e4dd3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the Quality of Retrieval from the VectorIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768664c-f6da-4870-a882-1affb219a2d1",
   "metadata": {},
   "source": [
    "Instantiate a RetrieverEvaluator with the metrics that we want to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99063386-621d-4ee7-93e6-17ab95e49839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [\"hit_rate\", \"mrr\", \"precision\", \"recall\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(metrics, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26ffc053-eab3-42f8-8b21-d8abcc5e19e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the historical consistency requirements for armor equipment according to the given document?\n",
      "Metrics: {'hit_rate': 0.0, 'mrr': 0.0, 'precision': 0.0, 'recall': 0.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on a single query\n",
    "# The output is verbose, but may be useful for looking at specific results\n",
    "\n",
    "query_id = 1  # change this to math the query id of interest\n",
    "\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[query_id]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abf98b03-9305-46f3-a9fe-52cbd1d1cd97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalEvalResult(query='What are the historical consistency requirements for armor equipment according to the given document?', expected_ids=['node_1'], expected_texts=None, retrieved_ids=['node_3', 'node_15'], retrieved_texts=['2. Historical ConsistencyRequirements\\n2.1.Equipmentfromhistoricalsources● Onlyarmorsderivedfromhistorical sourcesarepermittedforuse.● Armorsmust alignwithAuthenticityRulesdocuments.\\n2.2.Datesofsources● Armorsmust alignwithsourcesdatingbetweenthe14th(1300)and17th(1600)centuries.● For safety considerations, reproductions of armors predating the 13th century areprohibited.\\n2.3.Prohibitedfeatures● Prohibited features encompass evident indications of modern materials or manufacturingtechniques including: neoncolors, obvious nyloncords, plastic ties, visibleweldedseams,heat-induceddiscoloration, modernfootwearandothervisiblemodernequipment.\\n2.4.ConsistencyinEquipmentArmors, shields and weapons must consist of pieces fromthesamestyle. Distinct styles aredefinedinmodern-daytermsas:\\n2.4.1.WesternEuropestyle● 14thcentury:from1300to1380● Transitional:from1380to1420● 15th century: from1420 to 1500. XVth stylearmor must beapprovedby theAuthenticity Committee. We recommended seeking approval before buyingsucharmor.● Western Europe includes the following modern countries: Great Britain,Ireland, France, Portugal, Spain, Germany, Italy, Norway, Denmark, Sweden,Finland, Austria, Switzerland, Belgium, andtheNetherlands2.4.2.SlavicInfluence● Central Europe14th:from1300to1400● Russianlatearmors:from1500to1700● SlavicInfluenceincludesthefollowingmoderncountries:CzechRepublic,Romania, Hungary, Poland, Slovakia, Slovenia, Croatia, Latvia, Estonia,Moldova, Serbia, Ukraine, Russia, andBelarus.2.4.3.Easterninfluence● Chinesestyle: from1300to1600● Japanesesamurai style: from1400to1700● Middle-East style: from1300to1700● Easterninfluenceincludes thefollowingmoderncountries: China, Japan, India,Korea, Iran, Iraq, Turkey, Egypt\\nTechnicalRequirementsforarmors,weaponsandshields 3', '2.Historical ConsistencyRequirements\\n2.1.Equipmentfromhistorical sources● Onlyweaponsderivedfromhistorical sourcesarepermittedforuse.\\n● Weaponsmust alignwithAuthenticityRulesdocuments.\\n2.2.Datesofsources● Weapons must align with sources dating between the 14th (1300) and 17th (1600)centuries.\\n● For safety considerations, reproductions of armors predating the 13th century areprohibited, asareweaponsof thesameperiod.\\n2.3.Prohibitedfeatures● Prohibited features encompass evident indications of modern materials or manufacturingtechniques including: neoncolors, obvious nyloncords, plastic ties, visibleweldedseams,heat-induceddiscolorationandothervisiblemodernequipment.\\n2.4.ConsistencyinEquipmentArmors, shields and weapons must consist of pieces from the same style. Distinct styles aredefinedinmodern-daytermsas:\\n2.4.1. WesternEuropestyle● 14thcentury: from1300to1380\\n● Transitional: from1380to1420\\n● 15th century: from1420 to 1500. XVth style armor must be approvedby theAuthenticityCommittee. Werecommendedseekingapproval beforebuyingsucharmor.\\n● Western Europe includes the following modern countries: Great Britain, Ireland, France,Portugal, Spain, Germany, Italy, Norway, Denmark, Sweden, Finland, Austria, Switzerland,Belgium, andtheNetherlands\\n2.4.2. SlavicInfluence● Central Europe14th: from1300to1400\\n● Russianlatearmors: from1500to1700\\n● Slavic Influence includes the following modern countries: Czech Republic, Romania,Hungary, Poland, Slovakia, Slovenia, Croatia, Latvia, Estonia, Moldova, Serbia, Ukraine,Russia, andBelarus.'], mode=<RetrievalEvalMode.TEXT: 'text'>, metric_dict={'hit_rate': RetrievalMetricResult(score=0.0, metadata={}), 'mrr': RetrievalMetricResult(score=0.0, metadata={}), 'precision': RetrievalMetricResult(score=0.0, metadata={}), 'recall': RetrievalMetricResult(score=0.0, metadata={})})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see detail on which nodes were returned, etc, we can look at the whole returned object\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e410697-2972-4b9f-8f8d-94d477afdc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Run evaluation on the entire test dataset (autogenerated above)\n",
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20614f76-9896-4253-8a5e-e101a1f082ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_results(name, eval_results):\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "    hit_rate = full_df[\"hit_rate\"].mean()\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    precision = full_df[\"precision\"].mean()\n",
    "    recall = full_df[\"recall\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame({\"retrievers\": [name],\n",
    "                              \"hit_rate\": [hit_rate], \"mrr\": [mrr],\n",
    "                              \"precision\": [precision], \"recall\": [recall],\n",
    "                             })\n",
    "    return metric_df, full_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1159cf-271e-46b2-8264-4bd89b39b648",
   "metadata": {},
   "source": [
    "### Top-level Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42016d73-4f1a-4d09-adc0-5bd8a8ffda85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retrievers</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>top-2 eval</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.730769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retrievers  hit_rate       mrr  precision    recall\n",
       "0  top-2 eval  0.730769  0.596154   0.365385  0.730769"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary, detail = display_results(f\"top-{number_of_items_to_return} eval\", eval_results)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ec8877-bec8-4c50-b2f0-67ad6b7b5dab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hit_rate  mrr  precision  recall\n",
       "0        0.0  0.0        0.0     0.0\n",
       "1        0.0  0.0        0.0     0.0\n",
       "2        0.0  0.0        0.0     0.0\n",
       "3        1.0  1.0        0.5     1.0\n",
       "4        1.0  1.0        0.5     1.0\n",
       "5        1.0  0.5        0.5     1.0\n",
       "6        1.0  0.5        0.5     1.0\n",
       "7        1.0  1.0        0.5     1.0\n",
       "8        1.0  1.0        0.5     1.0\n",
       "9        1.0  1.0        0.5     1.0\n",
       "10       0.0  0.0        0.0     0.0\n",
       "11       1.0  1.0        0.5     1.0\n",
       "12       0.0  0.0        0.0     0.0\n",
       "13       1.0  0.5        0.5     1.0\n",
       "14       0.0  0.0        0.0     0.0\n",
       "15       1.0  0.5        0.5     1.0\n",
       "16       1.0  1.0        0.5     1.0\n",
       "17       1.0  1.0        0.5     1.0\n",
       "18       0.0  0.0        0.0     0.0\n",
       "19       1.0  0.5        0.5     1.0\n",
       "20       1.0  0.5        0.5     1.0\n",
       "21       1.0  1.0        0.5     1.0\n",
       "22       1.0  0.5        0.5     1.0\n",
       "23       1.0  1.0        0.5     1.0\n",
       "24       1.0  1.0        0.5     1.0\n",
       "25       1.0  1.0        0.5     1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, look at the detailed, question by question metrics:\n",
    "detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d393f-7b77-4de0-86a1-569fb4d6de5c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f08a3-317c-46fc-8ccb-3f0b133897aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fbf95-39a4-43e4-b1ce-8296f28c8737",
   "metadata": {},
   "source": [
    "**01** Run the notebook in full, with the default, provided, document set (one document in .txt file).\n",
    "\n",
    "Copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell.\n",
    "\n",
    "Save the notebook and rename is as `capstone-02-01-first-run.ipynb`. Download the notebook for assignment submission.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|--------|\n",
    "| Retriever Hit Rate | 0.636364 |\n",
    "| MRR | 0.534091 |\n",
    "| Precision | 0.318182 |\n",
    "| Recall | 0.636364 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a91685-af8e-40c3-9d75-8e427a227043",
   "metadata": {},
   "source": [
    "**02** Replace the default document with one of more of your own documents and re-run all the cells.\n",
    "\n",
    "- First make a copy of the first notebook and paste it into the same directory.\n",
    "- Delete the original content on source_docs\n",
    "- Delete the cell in the notebook that will download the original content again to source_docs\n",
    "- Upload your content to source_docs\n",
    "- Initially test with a small set - 20 to 40 nodes aka document chunks - to save you time as you experiment. This may mean that you delete some of your content, to reduce its size. \n",
    "- Run the whole notebook with your small set of  test documents and observe the results.\n",
    "\n",
    "Wrapping up:\n",
    "- Copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell.\n",
    "- The top-level results returned are your `baseline` results. As you experiment with different configurations, in the following assignment tasks, you will see the route to improved accuracy from this baseline.\n",
    "- Describe your test dataset in this cell. How many documents, how many chunks, what is the topic of the content, what is the language.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-02-my-data-baseline.ipynb`. Download the notebook for assignment submission.\n",
    "\n",
    "**Answer**\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|--------|\n",
    "| Retriever Hit Rate | 0.730769 |\n",
    "| MRR | 0.596154 |\n",
    "| Precision | 0.365385 |\n",
    "| Recall | 0.730769 |\n",
    "\n",
    "The test dataset is 2 PDF outlining armor and weapon requirements for medieval sport combat (also known as buhurt). I updated the code to retrieve the documents directly from the source using `wget`.\n",
    "\n",
    "The 2 PDF combined represents **25 pages**, which generated **26 nodes** using `chunk_size` of **512**.\n",
    "\n",
    "The PDFs are in **English**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc93a6-9787-4fe3-aa06-15d06fb3b668",
   "metadata": {},
   "source": [
    "**03** Using your data experiment with different embedding models\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-02-my-data-baseline.ipynb`, and paste it into the same directory.\n",
    "- Change which embedding model is going to be used from the original setting. \n",
    "There are three other options prepared in this notebook, see `KEY CELL 01`. \n",
    "- Run all the following cells in the notebook and note the in accuracy metrics for the selected model.\n",
    "- Repeat with one or more of the embeddings models, noting the accuracy metrics for the selected model, each time.\n",
    "Note: Changing the embeddings model will change the accuracy of the retriever. Exactly how much will depend on your content.\n",
    "\n",
    "Wrapping up:\n",
    "- For each embeddings model that you experiment with, copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell, along with an indication as to which embeddings model was being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which model was best, did you see a major improvement in the metrics, can you suggest a reason for why the best embeddings model is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-03-my-data-embeddings.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b739f-5af8-4719-8d69-ae70af9ed9c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**04** Using your data experiment with different ingestion pipelines\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-03-my-data-embeddings.ipynb`, and paste it into the same directory.\n",
    "- Make sure the the notebook is configured to use the best performing embeddings model.\n",
    "- Change which pipeline is going to be used from the original setting. \n",
    "There is one other options prepared in this notebook, see `KEY CELL 02`.\n",
    "- Run all the following cells in the notebook and note the in accuracy metrics for the selected pipeline.\n",
    "- Optionally, create your own pipeline and experiment with that to further improve the  accuracy metrics of your solution.\n",
    "Note: Changing the ingestion should change the accuracy of the retriever. Exactly how much will depend on your content. \n",
    "The two example pipelines in this notebook may not make much of a difference for your content. \n",
    "If you have time, you will learn most by experimenting with creating you own and seeing the change in the metrics.\n",
    "\n",
    "Wrapping up:\n",
    "- For each pipeline that you experiment with, copy the Top-level Evaluation Results (from the cell a little way above this one) into this cell, along with the definition of the pipeline being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which pipeline was best, did you see a major improvement in the metrics, can you suggest a reason for why the best pipeline is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-04-my-data-pipeline.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99fec15-e26a-4764-ba7e-db8e50d43d81",
   "metadata": {
    "tags": []
   },
   "source": [
    "**05** Using your data experiment with different values of k\n",
    "\n",
    "- First make a copy of the previous notebook,`capstone-02-04-my-data-pipeline.ipynb`, and paste it into the same directory.\n",
    "- Make sure the the notebook is configured to use the best performing pipeline and embeddings model.\n",
    "- Change the value of *k* that is going to be used from the original setting. \n",
    "There are two other options prepared in this notebook, see `KEY CELL 03`.\n",
    "- Run all the cells that follow below in the notebook and note the in accuracy metrics for the selected value of *k*. \n",
    "*Note*: with this setting, you do not need to re-run the cells above the cell where you make the update.\n",
    "\n",
    "\n",
    "Wrapping up:\n",
    "- For each value of *k* set for the `retriever`, run the evaluation and copy the Top-level Evaluation Results into this cell, along with noting the value of *k* being used.\n",
    "\n",
    "- Briefly describe your results in this cell. Which value of *k* gave the best results, did you see a major improvement in the metrics, can you suggest a reason for why the best pipeline is performing better than the worst.\n",
    "\n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-05-my-data-pipeline-with-k.ipynb`. Download the notebook for assignment submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a9be75-ae92-480b-b769-0432f1e2c87e",
   "metadata": {},
   "source": [
    "**06** Summarize\n",
    "\n",
    "- Breifly summarize, in four paragraphs what your learned, regarding the following topics:\n",
    "    - The configuration of the retriever for your content\n",
    "    - The process of experimentating with different settings and evaluating the results\n",
    "    - Which evaluation metric was most useful and why\n",
    "    - What might you do next, if you had a 40 hours or more to work on this, to further improve the quality of the retriever\n",
    "- Save your summary in this cell.\n",
    "    \n",
    "This completes this step.\n",
    "\n",
    "\n",
    "- Save and rename the notebook as `capstone-02-06-summary.ipynb`. Download the notebook for assignment submission.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6ea1c",
   "metadata": {},
   "source": [
    "Post each of the notebooks, individually, to submit your assignment. Do not zip the set of notebooks, as that makes it harder for the grading process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace4c60",
   "metadata": {},
   "source": [
    "## The following assignment tasks are completely optional \n",
    "The follow tasks are intended for students who want to dive deeper. They are more open ended and require changing and augmenting the code share above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8a7f5",
   "metadata": {},
   "source": [
    "**Optional Task 1** Text Chunking\n",
    "\n",
    "Experiment further with advanced chunking options and see if you can further improve the accuracy of the your Retriever (by having the better pre-processed data).\n",
    "\n",
    "Good candidates to look are [Semantic chunking](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/), and\n",
    "[Semantic double merging chunking](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_double_merging_chunking/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2610f",
   "metadata": {},
   "source": [
    "**Optional Task 2** Explore other Transformations\n",
    "\n",
    "Using the following [document](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/) as your starting point, consider which transformations would be most applicable to your set of documents and experiment and see the impact of those changes.\n",
    "\n",
    "There are lots of [transformations](https://docs.llamaindex.ai/en/v0.9.48/module_guides/loading/ingestion_pipeline/transformations.html) to consider including \n",
    "[text splitters](https://docs.llamaindex.ai/en/v0.9.48/module_guides/loading/node_parsers/modules.html#text-splitters),\n",
    "[node parsers](https://docs.llamaindex.ai/en/v0.9.48/module_guides/loading/node_parsers/modules.html), \n",
    "and [metadata extractors](https://docs.llamaindex.ai/en/v0.9.48/module_guides/loading/documents_and_nodes/usage_metadata_extractor.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825452f6",
   "metadata": {},
   "source": [
    "**Optional Task 3** Embeddings Model\n",
    "\n",
    "Experiment further with other state of the art embeddings model and see if you can further improve the accuracy of the your Retriever (by having the better pre-processed data).\n",
    "\n",
    "The embeddings models developed by [Voyage AI](https://www.voyageai.com) are some of the best in the industry. They also provide generous free-tier use of the embeddings models (as a service). Signing-up to get a developer account is a fairly light-weight process. \n",
    "\n",
    "The benefit that you'll get is 1/ seeing how to use a whole new model family for embeddings with Llamaindex, 2/ a deeper knowledge of your embeddings options, and 3/ perhaps, a more accurate Retriever for your solution.\n",
    "\n",
    "If you prefer to try other embeddings models, such as those provided by OpenAI, that's also well worth exploring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
