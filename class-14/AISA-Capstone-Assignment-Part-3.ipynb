{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ede245-8bd8-4ca3-8922-c3b725ba05ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AISA Capstone 3 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc35067-b7cf-438c-81ff-1eb4ed6a9725",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook provides an environment for you to build your intuition on the steps to take when developing a high quality Retrieval Augmented Generation (RAG) solution. \n",
    "RAG solutions retrieve data before calling the large language model (LLM) to generate an answer. \n",
    "The retrieved data is used to augment the prompt to the LLM by adding the relevant retrieved data in context. \n",
    "Any RAG solution is only as good as the quality of the data retrieval process. \n",
    "The AISA Capstone 2 assignment focused on retrieval accuracy for RAG.\n",
    "This notebook, follows on directly from that assignment, to focus on generating high-quality answers to question, \n",
    "and systematically assessing the quality of generated output.\n",
    "\n",
    "The RAG solution developed here is enabled by the Llamaindex framework. This is a popular framework in the industry for developing RAG and Agent based solutions. In addition to providing a core set of tools for orchestration of RAG and Agent workflows, there is broad integration with a variety of platforms for model inference (LLM, embedding, ...), and, importantly, tooling for solution evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4d6f9-7cc1-416b-b035-8403ca7bc8f0",
   "metadata": {},
   "source": [
    "## Prerequisites for running the notebook\n",
    "- That you have granted access to the Bedrock models that you are going to use, in the region (**us-west-2**) where you are going to use Bedrock - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html)\n",
    "- Your SageMakerExecutionRole has permissions to invoke Bedrock models - \n",
    "[reference](https://docs.aws.amazon.com/bedrock/latest/userguide/inference-prereq.html)\n",
    "- This notebook has been tested with SageMaker Notebook Instance running a `conda_python3` kernel\n",
    "- The AWS region set for Amazon Bedrock use, needs to be in a region where the models being used are 1/ available, and 2/ enabled for use. This notebook was tested with Bedrock region `us-west-2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5514f985-a013-48e9-acd9-73d4eeb137f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Implementation\n",
    "This notebook uses llamaindex to define and execute the RAG solution. We will be using the following tools:\n",
    "\n",
    "- **LLM (Large Language Model)**: e.g. Anthropic Claude Haiku available through Amazon Bedrock\n",
    "\n",
    "  LLMs are used in the notebook for 1/ RAG response generation, to show the overall RAG workflow in actions, and 2/ for generating test questions on the indexed content (llamaindex nodes) for retrieval evaluation.\n",
    "  \n",
    "- **Text Embeddings Model**: e.g. Amazon Titan Embeddings available through Amazon Bedrock\n",
    "\n",
    "  This embedding model is used to generate semantic vector representations of the content (llamaindex nodes) to be stored and the questions input to the RAG solution.\n",
    "  \n",
    "- **Document Loader**: SimpleDirectoryReader (Llamaindex)\n",
    "\n",
    "  Before your chosen LLM can act on your data you need to load it. The way LlamaIndex does this is via data connectors, also called 'Reader'. Data connectors ingest data from different data sources and format the data into Document objects. A Document is a collection of data (currently text, and in future, images and audio) and metadata about that data.\n",
    "  \n",
    "  This implementation use SimpleDirectoryReader, which creates documents out of every file in a given directory. It can read a variety of formats including Markdown, PDFs, Word documents, and PowerPoint decks.\n",
    "\n",
    "- **Vector Store**: VectorIndex (Llamaindex)\n",
    "\n",
    "  In this notebook we are using this in-memory vector-store to store both the embeddings and the documents. In an enterprise context this could be replaced with a persistent store such as AWS OpenSearch, RDS Postgres with pgVector, ChromaDB, Pinecone or Weaviate.\n",
    "  \n",
    "  LlamaIndex abstracts the underlying vector database storage implementation with a VectorIndex class. This warps the Index, which is a data structure composed of Document objects, designed to enable querying by an LLM. The Index is designed to be complementary to your querying strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe5196-2edf-4f54-be53-12bc4d70e3ea",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d293136b-753b-4774-95bc-cd965a38b13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Install required Python modules for constructing the RAG solution.\n",
    "You only need to run this once. \n",
    "\n",
    "Don't stress if you see an error in the output of the `pip install`. While this is concerning, it will likely not effect the functioning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c358c2-f00d-4582-867a-1ae49bfa9354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    llama-index \\\n",
    "    llama-index-llms-bedrock \\\n",
    "    llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da34a35-29ee-4c26-8398-7e1bfe7c58f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 1: Setting up the baseline configuration with some sample content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724a2d-6686-4177-baa1-3fed868170d5",
   "metadata": {},
   "source": [
    "Download the default RAG test source data to our target source_docs directory. \n",
    "You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190bed5-26d3-4897-a270-74087c9af1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_docs_dir = './source_docs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d14fc8-2f54-4a24-9ad4-9e84678e9495",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following creates the source_docs directory and downloads a document to that directory. The contents of this directory, \n",
    "initially the document that is downloaded here, will be used in the steps that follow.\n",
    "\n",
    "After running this notebook in its entirity and reviewing its operation, delete this content and add your own content to the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9c7c-3e23-40ba-a7c1-b852d083aa7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and load data\n",
    "!mkdir -p {source_docs_dir}\n",
    "!wget --no-check-certificate 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O {source_docs_dir}'/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab5b10-dcb8-423c-b1ef-d39a656f6b41",
   "metadata": {
    "tags": []
   },
   "source": [
    "Import auxilliarty modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f69ef6-0453-465f-adc0-3180b7f41352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import boto3  # AWS SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b34da-dd60-46b9-8bab-739c86f9b23e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is an output to screen helper method for making some output more easy to read.\n",
    "\n",
    "import textwrap\n",
    "from io import StringIO\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65d3264-223c-4902-8e21-6886e25a52b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is required when running within a jupyter notebook, otherwise you will get errors when llamaindex modules run\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b618759-983d-4d7e-8bf6-840b355b5609",
   "metadata": {},
   "source": [
    "Import required Python modules for constructing and evaluating the RAG solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025ceaf-9699-43ba-9369-773612652b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "from llama_index.core.evaluation import (\n",
    "    DatasetGenerator,\n",
    "    RetrieverEvaluator,\n",
    "    generate_question_context_pairs,\n",
    ")\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Response,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66520-b9b5-4fdc-a00b-45d23734bfdb",
   "metadata": {},
   "source": [
    "## Configure the models that will be used for the RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b8643-0fc7-4984-bfb4-820b92ce0e99",
   "metadata": {},
   "source": [
    "**Note**: By default this notebook with use the `us-west-2` region. This region has support for the models used in this notebook. You should not need to change this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946049d-010b-4485-83ce-08425c254261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AWS_REGION = \"us-west-2\"\n",
    "# AWS_REGION = \"us-east-1\"  # this is an alternative setting to use if desired "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a6109-6dba-4312-951a-af27765774b3",
   "metadata": {},
   "source": [
    "Define the set of Bedrock model IDs that we that we'll use when developing and testing our solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8d3b8-0a47-45db-9b2f-3265d0c795f0",
   "metadata": {},
   "source": [
    "Establish a connection to the Amazon Bedrock service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ca8a1-d4b7-463e-8770-76bf83a37246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boto3_bedrock = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d038da-c7fb-4c07-957c-c23a1effe13e",
   "metadata": {},
   "source": [
    "### Configure the target embeddings models for use with Llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2a047-82b9-4ee9-bbcc-4ab33d9a07f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_multilingual_v1_id = \"amazon.titan-embed-text-v1\"\n",
    "titan_text_embeddings_multilingual_v2_id = \"amazon.titan-embed-text-v2:0\"\n",
    "cohere_text_embeddings_english_id = \"cohere.embed-english-v3\"\n",
    "cohere_text_embeddings_multilingual_id = \"cohere.embed-multilingual-v3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942ca6ba-721d-4af6-a25f-0026218373c5",
   "metadata": {},
   "source": [
    "Configure our chosen embeddings model for use with llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac101f7-1f62-4720-9a94-64bdce9102de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titan_text_embeddings_v2 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v2_id,region_name=AWS_REGION)\n",
    "titan_text_embeddings_v1 = BedrockEmbedding(model=titan_text_embeddings_multilingual_v1_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_english = BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)\n",
    "cohere_text_embeddings_multilingual= BedrockEmbedding(model=cohere_text_embeddings_english_id,region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac963474-ee9e-47f4-ae7f-5896b3ee805d",
   "metadata": {},
   "source": [
    "### Configure the target LLMs for use with Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66fcc-a3c2-4acd-9004-53d96c083391",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following Mistral models can be used to produce questions for evaluation. \n",
    "The Titan model produces questions of lesser quality and sometimes not in the format needed by the tools. \n",
    "\n",
    "**Note** Most Bedrock LLMs do not *produce test questions* in a format that can be directly used for evaluation with the tooling as it is configured in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d60343-0fad-4fad-a6a0-1a28a60c5539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "titan_text_express_id = \"amazon.titan-text-express-v1\"\n",
    "claude_haiku_3_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "claude_sonnet_35_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66c9c9-752f-425c-b783-c5951c47d95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the parameters to be applied when invoking the model\n",
    "model_kwargs_llm = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 200,\n",
    "    \"max_tokens\": 4096\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241957a-b7e3-40d7-ada5-6bbf2a86394c",
   "metadata": {},
   "source": [
    "### NOTE: This notebook uses two additional LLMs !!\n",
    "You will need to enable use to the following models in the Bedrock console\n",
    "- Anthropic Claude Haiku 3\n",
    "- Anthropic Clause Sonnet 3.5\n",
    "\n",
    "This is in addition to the Mistral and Titan models used in Capstone 2.\n",
    "\n",
    "If these are no enabled you will encounter errors later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223cc97-dc5b-40f4-8acb-92be61e03885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_mistral7b = Bedrock(model=instruct_mistral7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_mixtral8x7b = Bedrock(model=instruct_mixtral8x7b_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_titan_express = Bedrock(model=titan_text_express_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_haiku_3 = Bedrock(model=claude_haiku_3_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)\n",
    "llm_sonnet_35 = Bedrock(model=claude_sonnet_35_id, client=boto3_bedrock, model_kwargs=model_kwargs_llm, region_name=AWS_REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babd1c3-cc16-4914-ba7a-83185c56a418",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the embeddings model to use for the cells that follow\n",
    "\n",
    "The embeddings model is a critical choice for the accuracy of your RAG solution.\n",
    "Experiment with the options here to see which is best for your content.\n",
    "If you want more, test with further alternatives. There are many that are readily supported by llama_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d75b4-f9ef-4398-9214-15920b917ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After the first run, set this to match you intended configuration based on your learning from Capstone II\n",
    "\n",
    "# embed_model = titan_text_embeddings_v1\n",
    "embed_model = titan_text_embeddings_v2\n",
    "# embed_model = cohere_text_embeddings_english\n",
    "# embed_model = cohere_text_embeddings_multilingual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70d0f7-6744-41e8-a62b-8f5624fe54f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the LLM to use for the cells that follow\n",
    "The LLM will be used for question generation and RAG answer generation in this notebook as it is currently configured.\n",
    "The default value llm_mistral7b works well with the code and should be used if possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea65f7-1428-41c0-850d-c1eb176c41e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = llm_mistral7b\n",
    "# llm_model = llm_mixtral8x7b\n",
    "# llm_model = llm_titan_express\n",
    "# llm_model = llm_haiku_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bbe67d-5908-4d3f-8c0a-831391a4ad5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set LlamaIndex default model settings to what was set in the cells above\n",
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46229098-ce14-4834-a863-2035547a53ab",
   "metadata": {},
   "source": [
    "## Read in the documents for adding to our data store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fc1fd-f34b-41a1-9d88-e4647a521273",
   "metadata": {},
   "source": [
    "Read in the documents in the 'data/source_docs' directory into a structure ready for use by llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bd6d1-2454-45ff-a33e-fcbf3ea77867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = SimpleDirectoryReader(source_docs_dir)\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b952f0-8f8a-4920-ad35-e8abe08fc429",
   "metadata": {},
   "source": [
    "Quick check here to see that all of your documents were read. The count should match the number of pages in the documents in source_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752782ee-1d5c-41d7-8266-c1a6c9194a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261170a-c9db-4f59-b391-9e5039397e26",
   "metadata": {},
   "source": [
    "## Create and run the document ingestion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a1fda-bc75-42a1-be1b-54c29a624abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell defines two different document ingestion pipelines. \n",
    "If you have time, test using both of these, and create you own and test with that also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773aa337-a276-441a-92be-190a2d9f37e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define two transformation for the ingestion pipelines for initial experimentation\n",
    "\n",
    "transformations_00=[\n",
    "        TokenTextSplitter(separator=\" \", chunk_size=512, chunk_overlap=100),\n",
    "        embed_model,\n",
    "    ]\n",
    "\n",
    "transformations_01=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=100),\n",
    "        TitleExtractor(),\n",
    "        embed_model,\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b013b-f07e-441d-8396-dab15325e7d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use the following cell to configure the data ingestion pipeline for processing the source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134f8fb-03eb-4167-9063-4fcd08ed7643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After the first run, set this to match you intended configuration based on your learning from Capstone II\n",
    "\n",
    "pipeline = IngestionPipeline(transformations=transformations_00)\n",
    "# pipeline = IngestionPipeline(transformations=transformations_01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314c658-de88-432b-98bc-08a5ccf7bee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run the configured ingestion pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6cf10-0dcc-40eb-a659-3b70cfb4a939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=documents)\n",
    "print(f\"number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331bd97-75af-432b-a77c-a82be092585d",
   "metadata": {
    "tags": []
   },
   "source": [
    "This may make test analysis easier. It is non-essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b464f0b-b123-4a12-b85f-8718fcaee4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default, the node ids are set to random uuids. \n",
    "# To ensure same id's per run, we manually set them to consistent sequential numbers.\n",
    "for idx, node in enumerate(nodes):\n",
    "    node.id_ = f\"node_{idx}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb0df-bf84-4630-87b0-ba8fae23f461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate that node has an embedding associated with it\n",
    "for idx, node in enumerate(nodes):\n",
    "    if node.id_ == \"node_0\":\n",
    "        print(node.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2620ecd1-c357-49a0-8921-7de63903f870",
   "metadata": {},
   "source": [
    "## Create the VectorIndex \n",
    "This creates our vector database, in memory in this case,  using the nodes that were created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25602778-44d3-4adf-b3e2-68b679feeea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb1160-b3e8-445d-8680-fa9d46faea06",
   "metadata": {},
   "source": [
    "## Test that we have a valid starting point for our evaluation\n",
    "We run a quick system test with the defaul llama_index RAG workflow with a question that is relevant to our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3006bf-14de-4c74-93dd-3a584bcb5be9",
   "metadata": {},
   "source": [
    "Instantiate a query engine object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3b4be-5fe1-4789-b7b9-bf17cb6a9f77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine(llm=llm_mistral7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c163ad-7a8c-40e1-a9f4-8caac25e8dc1",
   "metadata": {},
   "source": [
    "Specify a question that has can be answered by the document(s) that have been ingested. For the default document, the following is a valid question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a8e95-22ae-45e2-9f9b-d0734d6a4bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"\"\"Based on Paul Graham's experience, why did he initially lose interest in studying philosophy\n",
    "and switch to AI in college?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479cecf-4725-45f6-be71-7cd5bf4cf915",
   "metadata": {},
   "source": [
    "Run the default RAG pipeline with the example query. This should give a meaningful result. Don't worry if the answer is overly verbose, etc. We'll fix that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8af86-4544-4286-be1a-a32b7fe55c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(example_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06687f-c443-4bdb-8e70-25686fbd3564",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127b15d-cb49-43ab-8f8e-a908915eb6c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate the retrieval accuracy of the VectorIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85730c7c-81f2-493c-8fe0-3a59204e2b7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a set of question and node (context) pairs to drive the tests that follow\n",
    "This uses the llm that give the methods and the document data stored in the nodes (created during document ingestion)\n",
    "\n",
    "This will make many calls to the specified LLM (num_questions_per_chunk * number of nodes). This will likely be throttled by Bedrock. The llama_index API will work through the throttling except in extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0e486-180b-42e3-8ca3-852e73690c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "qa_dataset = generate_question_context_pairs(nodes, num_questions_per_chunk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9794d-4890-42ca-867b-5558955932e9",
   "metadata": {},
   "source": [
    "Take a look at the sample queries generated. This should show a meaningful questions related to your document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29a7fc-22b4-4ff4-98d6-764280755a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for item in list(qa_dataset.queries.items()):\n",
    "    print(item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d10a4af-f49d-4eea-82ca-e7d5cca9b990",
   "metadata": {},
   "source": [
    "## Instantiate a retriever against the index for testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fad7c-f10c-46a3-8a9a-6d3e5c25b64d",
   "metadata": {},
   "source": [
    "### Set the number of items to return from the Retriever\n",
    "This is a trade-off item, more returned content is not always better. Consider how this may impact your pipeline and evaluation results and experiment with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66407c-ea89-4c90-8be0-1bc70c2700fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# After the first run, set this to match you intended configuration based on your learning from Capstone II\n",
    "# If you did not complete Capstone II then leave this as is\n",
    "\n",
    "# number_of_items_to_return = 2\n",
    "# number_of_items_to_return = 3\n",
    "number_of_items_to_return = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6db691-6af4-4856-8ad2-604c0c6f629c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=number_of_items_to_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab82df-ebaa-4a2f-a98b-298198672f26",
   "metadata": {},
   "source": [
    "Run a quick system test on the retriever and check that the output nodes look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf0400-0e17-4d7f-aee7-95546b8eaa4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"\"\"Based on Paul Graham's experience, why did he initially lose interest in studying philosophy\n",
    "and switch to AI in college?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6327e-46ca-4e1d-9979-11c86471d784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(example_query)\n",
    "print(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f47e2-1e69-4a8e-a26c-886ef2e4dd3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate the Quality of Retrieval from the VectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20614f76-9896-4253-8a5e-e101a1f082ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is a helper function to output the results of the evaluation\n",
    "\n",
    "def display_results(name, eval_results):\n",
    "    metric_dicts = []\n",
    "    for eval_result in eval_results:\n",
    "        metric_dict = eval_result.metric_vals_dict\n",
    "        metric_dicts.append(metric_dict)\n",
    "\n",
    "    full_df = pd.DataFrame(metric_dicts)\n",
    "    mrr = full_df[\"mrr\"].mean()\n",
    "    precision = full_df[\"precision\"].mean()\n",
    "    recall = full_df[\"recall\"].mean()\n",
    "\n",
    "    metric_df = pd.DataFrame({\"retrievers\": [name], \"mrr\": [mrr],\n",
    "                              \"precision\": [precision], \"recall\": [recall],\n",
    "                             })\n",
    "    return metric_df, full_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768664c-f6da-4870-a882-1affb219a2d1",
   "metadata": {},
   "source": [
    "Instantiate a RetrieverEvaluator with the metrics that we want to review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99063386-621d-4ee7-93e6-17ab95e49839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = [\"mrr\", \"precision\", \"recall\"]\n",
    "\n",
    "retriever_evaluator = RetrieverEvaluator.from_metric_names(metrics, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ffc053-eab3-42f8-8b21-d8abcc5e19e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate on a single query \n",
    "# The output is verbose, but may be useful for looking at specific results\n",
    "\n",
    "query_id = 1  # change this to math the query id of interest\n",
    "\n",
    "sample_id, sample_query = list(qa_dataset.queries.items())[query_id]\n",
    "sample_expected = qa_dataset.relevant_docs[sample_id]\n",
    "\n",
    "eval_result = retriever_evaluator.evaluate(sample_query, sample_expected)\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57dff4ff-0e7c-4006-bab6-241c40749d70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# to see detail on which nodes were returned, etc, we can look at the whole returned object\n",
    "eval_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b47bbc-49e5-4c76-9894-276fc2c93529",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run evaulation on the entire test dataset (autogenerated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e410697-2972-4b9f-8f8d-94d477afdc32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1159cf-271e-46b2-8264-4bd89b39b648",
   "metadata": {},
   "source": [
    "### Top-level Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42016d73-4f1a-4d09-adc0-5bd8a8ffda85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary, detail = display_results(f\"top-{number_of_items_to_return} eval\", eval_results)\n",
    "summary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "207ec94b-a906-49ca-97b6-a15747cade35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optionally, look at the detailed, question by question metrics:\n",
    "detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17827f44-aabe-4af1-ab51-3815917a2be1",
   "metadata": {},
   "source": [
    "----\n",
    "### This completes setting up the retriever and validating it's level of accuracy\n",
    "\n",
    "The following sections are the focus of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc90e07-f53e-4db5-87d8-e17d9d853cf4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84eb9c-5535-4bff-bd3d-637e57297817",
   "metadata": {},
   "source": [
    "## Automating Q&A Generation with LllamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f494f-d21f-4cdc-b1e9-5772fcc8e423",
   "metadata": {
    "tags": []
   },
   "source": [
    "LllamaInex provides tools designed to automatically generate datasets when provided with a set of documents to query. In the example below, we use the **RagDatasetGenerator** class to generate evaluation questions and reference answers(ground truth) from the source documents and the specified number of questions per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca9b3e-8dc0-459e-a448-80e410ae43e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0850bee-7091-41de-bc3a-223ab69ae686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator, LabelledRagDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e2376-c4d8-4b2e-86d9-846b5e8848e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import (\n",
    "    LabelledRagDataset,\n",
    "    CreatedBy,\n",
    "    CreatedByType,\n",
    "    LabelledRagDataExample,\n",
    "    BaseLlamaDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490427cc-b41e-451a-af8b-66f4e4579db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_generator = RagDatasetGenerator.from_documents(\n",
    "    documents=documents,\n",
    "    llm=llm_mixtral8x7b,\n",
    "    num_questions_per_chunk=1, # set the number of questions per nodes\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(f\"Number of nodes created: {len(dataset_generator.nodes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad80cff-f70e-40ff-979b-be554580feaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_questions = dataset_generator.generate_dataset_from_nodes()\n",
    "eval_questions.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9586c8-f08a-4145-871c-15d0c9deabc3",
   "metadata": {},
   "source": [
    "**Note**: The following cell saves the generated question and answers to a JSON file and so that we do not need to run\n",
    "the question generation process above multiple times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36de0dd-6835-4526-8906-a3988ea92160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_questions.save_json('eval_questions.json')\n",
    "print(f\"Saving {len(eval_questions.examples)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a2f02-f60a-47ca-be08-068a098222d9",
   "metadata": {},
   "source": [
    "Use the questions saved in the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac37af-8b39-4602-bd14-26c557faef23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpointed_eval_questions = LabelledRagDataset.from_json('eval_questions.json')\n",
    "print(f\"Restoring {len(checkpointed_eval_questions.examples)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190b7b7-eb9b-4422-8bc3-790bce28040d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the question set into a Pandas dataframe for ease of use for the cells that follow\n",
    "eval_questions_df = checkpointed_eval_questions.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47332466-d1fd-4af0-a7d0-be2941c16a6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Automated Pipeline evaluation with LlamaIndex evaluators\n",
    "\n",
    "In the sections below, we'll show 4 automated evaluations available throught LlamaIndex. However, there are some additional metrics out-of-the-box that can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/):\n",
    "\n",
    "1. **Faithfulness**: This metric verifies whether the final response is in agreement with (doesn't contradict) the retrieved document snippets.\n",
    "2. **Relevancy**: This metrics checks whether the response and retrieved content were relevant to the query.\n",
    "3. **Correctness**: This metric evaluates whether the generated answer is relevant and agreeing with a reference answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f165a-6dc7-4780-8d47-ad50ddd5df5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator, CorrectnessEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efc217-a620-4edd-870c-c64f25fea6a5",
   "metadata": {},
   "source": [
    "**Note**: Configuring the LLM to use as the evaluator (aka Judge) of the output content from the RAG pipeline. \n",
    "\n",
    "For this, it is typical to use an LLM that has higher benchmark ratings than the LLM used for content generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830e59d-e4f1-4a6e-b4f6-6dbbf172f86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluator_llm = llm_mixtral8x7b\n",
    "evaluator_llm = llm_sonnet_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0b1e15-1de8-4dfb-bdf1-71c13fc5c7c3",
   "metadata": {},
   "source": [
    "### Set up our default query engine for showing the baseline evaluation\n",
    "\n",
    "**Note** as the number of chunks (aka items) returned increased the size of the prompt increases and the smaller models may fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd61e7-88b1-4f50-942d-eee835927961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL #1\n",
    "\n",
    "llm_model = llm_mixtral8x7b\n",
    "# llm_model = llm_haiku_3\n",
    "# llm_model = llm_sonnet_35\n",
    "\n",
    "number_of_items_to_return = 3\n",
    "\n",
    "query_engine = vector_index.as_query_engine(llm=llm_model, similarity_top_k=number_of_items_to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e485c-0d73-4433-b3ae-24746ff4e006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "faithfulness_evaluator = FaithfulnessEvaluator(llm=evaluator_llm)\n",
    "relevancy_evaluator = RelevancyEvaluator(llm=evaluator_llm)\n",
    "correctness_evaluator = CorrectnessEvaluator(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c759d-98db-497e-9729-2b7243bef7e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Faithfulness to source documents\n",
    "\n",
    "The **Faithfulness** metric evaluates the coherence between the generated response and the source document snippets retrieved during the search process. This assessment is essential for identifying any discrepancies or hallucinations introduced by the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90a367-3353-4b2a-acf3-dc892eb07dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for evaluating the faithfulness of the output of a specific test case\n",
    "\n",
    "def evaluate_faithfulness_for_question(rag_engine, questions_df, question_number):\n",
    "\n",
    "    eval_question = questions_df.iloc[0,0]\n",
    "    response_vector = rag_engine.query(eval_question)\n",
    "\n",
    "    eval_result = faithfulness_evaluator.evaluate_response(response=response_vector)\n",
    "\n",
    "    print(\"Question: ----------------\")\n",
    "    print_ww(eval_question)\n",
    "    print(\"\\nAnswer: ----------------\")\n",
    "    print_ww(response)\n",
    "    print(\"\\n----------------\")\n",
    "\n",
    "    print_ww(\"Evaluation Result:\", eval_result.passing)\n",
    "    print_ww(f\"Reasoning:\\n{eval_result.feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8f443-f563-4a84-918a-f00256e44db9",
   "metadata": {},
   "source": [
    "Take a look at this evaluation in action by seeing the content inputs and outputs for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1bffe-8432-417e-a852-1345b930992f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_number = 0\n",
    "evaluate_faithfulness_for_question(query_engine, eval_questions_df, question_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6f1fc-26ec-4322-913a-18330b06eedd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Relevancy of response + source nodes to the query\n",
    "\n",
    "The **Relevancy** metric verifies the correspondence between the response and the retrieved source documents with the user's query. This evaluation is crucial for assessing whether the response properly addresses the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74eb87-f433-45d2-bb66-0e24c19490ad",
   "metadata": {},
   "source": [
    "The **Relevancy Evaluator** module is useful to measure if the response + source nodes match the query. Therefore, it helps measuring if the query was actually answered by the response. In this example, as the context information does not provide any details about the launch date of Amazon Bedrock Studio, then the evaluation result is **FALSE**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7ab6e-6032-48b1-896d-9994b4384a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for evaluating the relevancy of the output of a specific test case\n",
    "\n",
    "def evaluate_relevancy_for_question(rag_engine, questions_df, question_number):\n",
    "\n",
    "    eval_question = questions_df.iloc[question_number,0] \n",
    "    response_vector = rag_engine.query(eval_question)\n",
    "\n",
    "    eval_result = relevancy_evaluator.evaluate_response(\n",
    "        query=eval_question, response=response_vector\n",
    "    )\n",
    "\n",
    "    # print results\n",
    "    print(\"\\n--------- Question ---------\")\n",
    "    print_ww(eval_question)\n",
    "    print(\"\\n--------- Response ---------\")\n",
    "    print_ww(str(response_vector))\n",
    "    print(\"\\n--------- Passed ---------\")\n",
    "    print_ww(str(eval_result.passing))\n",
    "    print(\"\\n--------- Feedback ---------\")\n",
    "    print_ww(str(eval_result.feedback))\n",
    "    print(\"\\n--------- Source ---------\")\n",
    "    print_ww(response_vector.source_nodes[0].node.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5768e4d-3134-4f6f-9709-c06c5ea2fce7",
   "metadata": {},
   "source": [
    "Testing the first generated evaluation question with the **RelevancyEvaluator** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4fa376-8a37-423f-9fde-1a112ef37adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_number = 0\n",
    "evaluate_relevancy_for_question(query_engine, eval_questions_df, question_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb49dfe-47a3-49f2-979e-eb00e5bb6154",
   "metadata": {},
   "source": [
    "### Correctness of response for the query\n",
    "\n",
    "The **Correctness** metric checks the correctness of a question answering system, relying on a provided reference answer(\"ground truth\"), query, and response. It assigns a score from 1 to 5 (with higher values indicating better quality) alongside an explanation for the rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c872c3-cdd9-4569-8d18-96387e70e61d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for evaluating the relevancy of the output of a specific test case\n",
    "\n",
    "def evaluate_correctness_for_question(rag_engine, questions_df, question_number):\n",
    "\n",
    "    eval_question = questions_df.iloc[question_number, 0]\n",
    "    ground_truth = questions_df.iloc[question_number, 2]\n",
    "\n",
    "    response_vector = rag_engine.query(eval_question)\n",
    "    generated_answer = str(response_vector)\n",
    "\n",
    "    correctness_results = correctness_evaluator.evaluate(\n",
    "                query=eval_question,\n",
    "                response=generated_answer,\n",
    "                reference=ground_truth\n",
    "            )\n",
    "\n",
    "    # print results\n",
    "    print(\"\\n--------- Question ---------\")\n",
    "    print_ww(eval_question)\n",
    "    print(\"\\n--------- Response ---------\")\n",
    "    print_ww(generated_answer)\n",
    "    print(\"\\n--------- Passed ---------\")\n",
    "    print_ww(str(correctness_results.passing))\n",
    "    print(\"\\n--------- Feedback ---------\")\n",
    "    print_ww(str(correctness_results.feedback))\n",
    "    print(\"\\n--------- Ground Truth ---------\")\n",
    "    print_ww(ground_truth)\n",
    "    print(\"\\n--------- Source ---------\")\n",
    "    print_ww(response_vector.source_nodes[0].node.get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dd112-be55-4508-9020-b8bf41bd4794",
   "metadata": {},
   "source": [
    "The following cell shows an example of the correctness_evaluator being applied to a specific question. this is by way of the `evaluate_correctness_for_question` function created above.\n",
    "\n",
    "This function will be useful when you want to dive deeper into understanding why a test is not passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a30d2-b3fb-44b0-be5a-8535c1696b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question_number= 0\n",
    "evaluate_correctness_for_question(query_engine, eval_questions_df, question_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561d0b3-0e67-4570-bbc3-3d7135d76a0a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f31b7-542a-47ce-972a-00f74b1c89ac",
   "metadata": {},
   "source": [
    "### Setup for run of the full test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82958a-a5d5-4d6f-b21e-4354fa55eb90",
   "metadata": {},
   "source": [
    "The following function presents the results in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71881b0-3804-44bc-b20c-e165ac73e775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Response\n",
    "import pandas as pd\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(query: str, response: Response, eval_result: str) -> None:\n",
    "\n",
    "    eval_df = pd.DataFrame(columns=['Query', 'Response', 'Source', 'Evaluation Result'])\n",
    "        \n",
    "    new_record = {\n",
    "                    \"Query\": query,\n",
    "                    \"Response\": str(response),\n",
    "                    \"Source\": (\n",
    "                        response.source_nodes[0].node.get_content()[:250] + \"...\"\n",
    "                    ),\n",
    "                    \"Evaluation Result\": eval_result,\n",
    "                }\n",
    "    eval_df = eval_df._append(new_record, ignore_index=True)\n",
    "\n",
    "\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde130b-8dc6-45f1-9227-db8443c3e186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This helper function will run the full set of tests and return the results\n",
    "\n",
    "from time import sleep\n",
    "sleep_number = 30\n",
    "\n",
    "def sleep_and_note_location(sec, loc):\n",
    "    print(f\"location: {loc}\")\n",
    "    sleep(sec)\n",
    "\n",
    "def run_evaluations(evaluation_dataset: pd.DataFrame, query_engine, evaluator_model):\n",
    "    \"\"\"Run a batch evaluation on a list of questions and reference answers using a provided query engine.\n",
    "\n",
    "    Args:\n",
    "        evaluation_dataset (DataFrame): A list of questions and reference_answers(ground truth) to evaluate.\n",
    "        query_engine (BaseQueryEngine): The query engine to use for answering the questions.\n",
    "        evaluator_model (LLM): The language model to use for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the evaluation results, including the query,\n",
    "            generated answer, faithfulness evaluation, and relevancy evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    results_list = []\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(llm=evaluator_model)\n",
    "    relevancy_evaluator = RelevancyEvaluator(llm=evaluator_model)\n",
    "    correctness_evaluator = CorrectnessEvaluator(llm=evaluator_model)\n",
    "\n",
    "    #for question, ground_truth in zip(evaluation_questions, evaluation_ground_truth):\n",
    "    for index, row in evaluation_dataset.iterrows():\n",
    "\n",
    "        print(f\"processing test case: {index + 1} / {len(evaluation_dataset)}\")\n",
    "\n",
    "        question = row['query']  \n",
    "        ground_truth = row['reference_answer']\n",
    "        \n",
    "        response = query_engine.query(question)\n",
    "        generated_answer = str(response)\n",
    "        sleep_and_note_location(sleep_number, \"faithfulness_evaluator\")\n",
    "\n",
    "        # Faithfulness evaluator\n",
    "        faithfulness_results = faithfulness_evaluator.evaluate_response(response=response)\n",
    "        sleep_and_note_location(sleep_number, \"relevancy_evaluator\")\n",
    "\n",
    "        # RelevancyEvaluator evaluator\n",
    "        relevancy_results = relevancy_evaluator.evaluate_response(query=question, response=response)\n",
    "        sleep_and_note_location(sleep_number, \"correctness_evaluator\")\n",
    "        \n",
    "        # CorrectnessEvaluator evaluator\n",
    "        correctness_results = correctness_evaluator.evaluate(\n",
    "            query=question,\n",
    "            response=generated_answer,\n",
    "            reference=ground_truth\n",
    "        )\n",
    "        sleep_and_note_location(sleep_number, \"end of iteration\")\n",
    "\n",
    "        current_evaluation = {\n",
    "            \"query\": question,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"faithfulness\": faithfulness_results.passing,\n",
    "            \"faithfulness_feedback\": faithfulness_results.feedback,\n",
    "            \"faithfulness_score\": faithfulness_results.score,\n",
    "            \"relevancy\": relevancy_results.passing,\n",
    "            \"relevancy_feedback\": relevancy_results.feedback,\n",
    "            \"relevancy_score\": relevancy_results.score,\n",
    "            \"correctness\": correctness_results.passing,\n",
    "            \"correctness_feedback\": correctness_results.feedback,\n",
    "            \"correctness_score\": correctness_results.score,\n",
    "        }\n",
    "        results_list.append(current_evaluation)\n",
    "        print(f\"processed test case: {index + 1} / {len(evaluation_dataset)}\")\n",
    "\n",
    "    evaluations_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    aggregate_results = {\n",
    "        'number_of_test_cases': len(evaluations_df),\n",
    "        'mean_faithfulness_score': round(evaluations_df['faithfulness_score'].mean(), 3),\n",
    "        'mean_relevancy_score': round(evaluations_df['relevancy_score'].mean(), 3),\n",
    "        'mean_correctness_score': round(evaluations_df['correctness_score'].mean(), 3)\n",
    "    }\n",
    "\n",
    "    return evaluations_df, aggregate_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7a767-e726-4450-920e-5d7684e91f42",
   "metadata": {},
   "source": [
    "**Note**: The throttling delays implemented by Bedrock significantly slow the test process\n",
    "\n",
    "It make take 4 minutes or more for a single test run. The default configuration will take at least 2 minutes.\n",
    "\n",
    "Have a break while this is running and limit your runs to 10-30 test cases, except for final runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cb0f7-4a9f-4c0e-9c76-afaad829f3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# KEY CELL: Running the configured evaluations with the generated test set\n",
    "\n",
    "# Run evaluations for the first n rows of the generated test set only\n",
    "n = 3\n",
    "evaluation_results_df, aggregate_results = run_evaluations(eval_questions_df.head(n), query_engine, evaluator_llm)\n",
    "evaluation_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214e7ce7-7de4-4e38-ad69-95bf46d46353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aggregate_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47985aec-3b81-4f66-8b95-f1752164133f",
   "metadata": {},
   "source": [
    "----\n",
    "## Pause\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c2343-eb25-451a-84ff-5775681469a5",
   "metadata": {},
   "source": [
    "# Assignment Task #1: Baseline: Using your configuration and documents\n",
    "\n",
    "Update the notebook to match your configuration for Capstone 2\n",
    "\n",
    "- Use your document set (rather than the canned/biographic dataset provided with this example)\n",
    "- Use the embeddings model that was best for your document set\n",
    "- Use the ingestion pipeline that was best for your document set\n",
    "\n",
    "If for some reason you did not complete Capstone 2, but you are completing Capstone 3, then note that use the content provided here.\n",
    "\n",
    "Once you have updated your configuration, rerun the notebook to this point.\n",
    "\n",
    "Answer the following questions in this cell:\n",
    "\n",
    "1. What are the aggregate evaluation scores for your configuration?\n",
    "2. Of the three evaluation measures, which one needs to be improved the most from your point of view and why?\n",
    "3. Look at a two of the failed test cases, using the evaluation functions that show the detail outputs, and see why the test case failed. For each of the two queries, note both the test case query, and your reasoning as to its failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2587f-4235-4cc7-b960-717489dd1d5b",
   "metadata": {},
   "source": [
    "----\n",
    "# Assignment Task #2: Experiment with the LLM for answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff8b66-e213-492b-b288-fa18babf3609",
   "metadata": {},
   "source": [
    "Change the LLM model configuration for the query engine in the next two celles. \n",
    "The notebook up to this point will have been using the `Claude Haiku 3` model. \n",
    "\n",
    "Change the configuration of the query engine to use the **one** or **two** other models that have been configured for use already.\n",
    "\n",
    "Then rerun the evaluations with the set of generated test cases.\n",
    "For each model review the difference in the aggregate score and in the quality of the output text.\n",
    "\n",
    "Answer the following questions in this cell:\n",
    "\n",
    "1. Which model was best for your content and what were its scores?\n",
    "2. Which model was worst for your content and what were its scores?\n",
    "3. Summarize the difference that in output quality that you observed between the best and the worst performing LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa037a3-f1e7-4322-87d0-8d857232076f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL \n",
    "\n",
    "query_engine_llm = llm_mixtral8x7b        # The `default` model for this notebook - used until you change this setting\n",
    "# query_engine_llm = llm_mistral7b        \n",
    "# query_engine_llm = llm_haiku_3         \n",
    "# query_engine_llm = llm_sonnet_35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc22d9c-5e89-4219-9bbe-2db98e442696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KEY CELL\n",
    "\n",
    "# After you update the query_engine configuration, then go back and re-run the test cases\n",
    "\n",
    "query_engine = vector_index.as_query_engine(llm=query_engine_llm, similarity_top_k=3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e1acb-00ff-419d-b697-c2c547994713",
   "metadata": {},
   "source": [
    "-----\n",
    "# Assignment Task #3: Experimenting with changing the prompt\n",
    "\n",
    "The default prompt provided by Llamaindex works quite well, but you can almost certainly do better. \n",
    "\n",
    "1. The cells below will change the default prompt to one that works better with the default content. Read that updated prompt and suggest two reasons why it might perform better the default prompt (refer back to the prompt engineering assignment).\n",
    "2. Update the alternative prompt to better match the topic and goals of your RAG solution, and update the query_engine with the cells that follow. Then rerun the tests and experiment further to improve your prompt. It will help to look at the output for specific queries, to get a deeper sense of the changes driven by your prompt.\n",
    "3. What are the final test metrics that you are getting for you configuration? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9886358-983b-4f0e-8814-f36931fd4696",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Take a look at the default prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e1d82-95d3-40b8-94fe-c7acdd1fbf3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812ced9-a7a9-4e2e-b9a4-df25292566b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define prompt viewing function for the prompt we care about\n",
    "prompt_template_key = \"response_synthesizer:text_qa_template\"\n",
    "\n",
    "def get_response_synthesizer_text_qa_prompt(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        if k == \"response_synthesizer:text_qa_template\":\n",
    "            return p.get_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01a815-4318-49d8-8fe5-257950937449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_prompt = get_response_synthesizer_text_qa_prompt(query_engine.get_prompts())\n",
    "print(default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a2ddb-716e-4eb8-9102-37afe853e757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_query=\"\"\"Based on Paul Graham's experience, why did he initially lose interest in studying philosophy\n",
    "and switch to AI in college?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cd833-b4fe-4dd2-8615-f29af42f9415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(example_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687d8ca-b296-464b-b830-88db63d0e29b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example alternate prompt\n",
    "\n",
    "# The objective being, for this example, to get a more concise and clear answer\n",
    "new_text_qa_prompt_str = (\n",
    "    \"You are an expert at book editor.\\n\"\n",
    "    \"Your task is to answer readers questions on the given information context,\"\n",
    "    \"in a clear, consise and friendly, manner, in two or three sentences.\\n\"\n",
    "    \"If the answer to their question is not available from the context,\"\n",
    "    \"reply that the question cannot be answered given the information that you have.\"\n",
    "    \"Output the answer directly without a preamble,\"\n",
    "    \"(e.g. without saying `Based on the context,` or similar).\"\n",
    "    \"<context>\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"</context>\\n\"\n",
    "    \"Given the context and not prior knowledge, \"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168c84b3-bcd8-4886-ba54-664a419418af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the qa_prompt to the new prompt\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {prompt_template_key: PromptTemplate(new_text_qa_prompt_str)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2101271-b641-43b5-a8ee-36c39f0251ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(example_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a7c045-7674-4c16-92dd-e7da43ebebe4",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Note** Once you have updated the query_engine with your prompt then re-run the tests"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d910612e-525d-427c-aa97-42aa934b862d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# reset to the default prompt (if desired) to see the difference once more\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {prompt_template_key: PromptTemplate(default_prompt)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d393f-7b77-4de0-86a1-569fb4d6de5c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6ea1c",
   "metadata": {},
   "source": [
    "-----\n",
    "# Assignment Task #4: Wrapping Up\n",
    "\n",
    "1. Do you think your customer would be satisified with the results? If there were not, what would you offer to do? \n",
    "2. We have been testing using 3 of the available end to end RAG evaluation methods supported by Llamaindex, which one or two might you also include for you customer, and why?\n",
    "3. In two or three sentences, note how you might further experiment and improve your RAG pipeline, if your customer gave you more money to make it better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace4c60",
   "metadata": {},
   "source": [
    "## The following assignment tasks are completely optional \n",
    "The follow tasks are intended for students who want to dive deeper. They are more open ended and require changing and augmenting the code share above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8a7f5",
   "metadata": {},
   "source": [
    "**Optional Task 1** Configure the query_engine to use an LLM from another LLM service provider and re-run the tests.\n",
    "\n",
    "You may be able to reduce the sleep_number for throttling to speed up your testing, depending on the LLM service\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2610f",
   "metadata": {},
   "source": [
    "**Optional Task 2** Add a reranking capability to the query engine\n",
    "\n",
    "Adding an LLM Reranker to the query engine only requires a few lines of code and will increase you solution accuracy while reducing inference costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18bb764",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
